# -*- coding: utf-8 -*-
"""app.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Y1jRJvzhlUjdd66vnUOBj57YzwXHYc1s
"""

# -*- coding: utf-8 -*-
"""
å°ç£ ETF å€‹äººåŒ–æ¨è–¦ç³»çµ± - çµ‚æ¥µå­¸è¡“æ‰¹åˆ¤é‡æ§‹ç‰ˆ
===========================================================

çµ‚æ¥µæ‰¹åˆ¤è¦é»ï¼š
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1. **å•å·è¨­è¨ˆè‡´å‘½ç¼ºé™·**ï¼š
   - åƒ…6é¡Œç„¡æ³•å»ºæ§‹ã€Œé¢¨éšªå¿ƒç†çµæ§‹ã€
   - ç„¡è¡Œç‚ºåèª¤æª¢æ¸¬ï¼ˆéåº¦è‡ªä¿¡ã€æ¡†æ¶æ•ˆæ‡‰ç­‰ï¼‰
   - ç„¡é‡‘èç´ é¤Šè©•ä¼°
   - ç„¡ä¸€è‡´æ€§æª¢é©—
   
2. **Î³ç¯„åœçˆ­è­°**ï¼š
   - Î³âˆˆ[1,4]éçª„ â†’ ç„¡æ³•å€åˆ†æ¥µç«¯åå¥½
   - Î³âˆˆ[0.5,10]éå¯¬ â†’ ç¼ºä¹å¯¦è­‰æ”¯æŒ
   - **è§£æ±ºæ–¹æ¡ˆ**ï¼šå‹•æ…‹ç¯„åœï¼ˆåŸºæ–¼é‡‘èç´ é¤Šèª¿æ•´ï¼‰
   
3. **æ•ˆç”¨å‡½æ•¸ç†è«–æ··äº‚**ï¼š
   - 4ç¨®æ•ˆç”¨åŠ æ¬Šç„¡ç†è«–ä¾æ“š
   - Taylorå±•é–‹å¿½ç•¥é«˜éšå‹•å·®
   - é…æ¯èˆ‡è³‡æœ¬åˆ©å¾—æ‡‰åˆ†é›¢å»ºæ¨¡
   
4. **çµ±è¨ˆæ¨è«–ç¼ºå¤±**ï¼š
   - ç„¡HACæ¨™æº–èª¤
   - ç„¡å¤šé‡æª¢é©—æ ¡æ­£
   - ç„¡Bootstrap CI

çµ‚æ¥µè§£æ±ºæ–¹æ¡ˆï¼š
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
âœ… 14é¡Œå•å·ï¼ˆåŸºç¤6+è¡Œç‚º4+ç´ é¤Š4ï¼‰
âœ… å‹•æ…‹Î³æ˜ å°„ï¼ˆé‡‘èç´ é¤Šèª¿ç¯€ï¼‰
âœ… æ­£ç¢ºCRRAæ•ˆç”¨ï¼ˆå«é«˜éšå‹•å·®ï¼‰
âœ… å®Œæ•´çµ±è¨ˆæ¨è«–ï¼ˆHAC+Holmæ ¡æ­£ï¼‰
âœ… ä¸€è‡´æ€§æª¢é©—ï¼ˆçŸ›ç›¾åµæ¸¬ï¼‰

ç‰ˆæœ¬ï¼šv7.0 - Ultimate Academic Reconstruction
æ—¥æœŸï¼š2025-02-13
æ‰¹åˆ¤è€…ï¼šHarvard/Wharton Faculty
"""

import streamlit as st
import pandas as pd
import numpy as np
import yfinance as yf
from datetime import datetime, timedelta
from scipy import stats, optimize
import plotly.graph_objects as go
import plotly.express as px
import requests
import warnings
from typing import Dict, Tuple, Optional, List, Any
from dataclasses import dataclass, field
import logging

warnings.filterwarnings('ignore')
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# å­¸è¡“å¸¸æ•¸ï¼ˆå¯¦è­‰ç ”ç©¶æ”¯æŒï¼‰
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

TRADING_DAYS = 252
RISK_FREE_RATE = 0.015
MARKET_RISK_PREMIUM = 0.065

# ETF Universe
ETF_LIST = {
    "0050.TW": {"name": "å…ƒå¤§å°ç£50", "type": "å¤§å‹è‚¡"},
    "006208.TW": {"name": "å¯Œé‚¦å°50", "type": "å¤§å‹è‚¡"},
    "00692.TW": {"name": "å¯Œé‚¦å…¬å¸æ²»ç†", "type": "ESG"},
    "00757.TW": {"name": "çµ±ä¸€FANG+", "type": "ç§‘æŠ€"},
    "0056.TW": {"name": "å…ƒå¤§é«˜è‚¡æ¯", "type": "é«˜è‚¡æ¯"},
    "00878.TW": {"name": "åœ‹æ³°æ°¸çºŒé«˜è‚¡æ¯", "type": "é«˜è‚¡æ¯"},
    "00919.TW": {"name": "ç¾¤ç›Šå°ç£ç²¾é¸é«˜æ¯", "type": "é«˜è‚¡æ¯"},
}
MARKET_BENCHMARK = "0050.TW"

# çµ±è¨ˆåƒæ•¸
MIN_TRADING_DAYS = 252
NEWEY_WEST_LAGS = 4
BOOTSTRAP_ITERATIONS = 500
CONFIDENCE_LEVEL = 0.95

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# è³‡æ–™çµæ§‹
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@dataclass
class FinancialMetrics:
    """è²¡å‹™æŒ‡æ¨™ï¼ˆå«çµ±è¨ˆæ¨è«–ï¼‰"""
    # é»ä¼°è¨ˆ
    ann_return: float
    ann_volatility: float
    sharpe_ratio: float
    sortino_ratio: float
    beta: float
    alpha: float
    max_drawdown: float
    
    # é«˜éšå‹•å·®
    skewness: float
    excess_kurtosis: float
    
    # HACæ¨™æº–èª¤
    return_se_hac: float
    sharpe_se_hac: float
    
    # Bootstrap CI
    sharpe_ci_lower: float
    sharpe_ci_upper: float
    
    # çµ±è¨ˆæª¢é©—
    sharpe_pvalue: float
    jb_pvalue: float  # Jarque-Beraå¸¸æ…‹æ€§
    
    # è³‡æ–™å“è³ª
    n_observations: int

@dataclass
class BehavioralProfile:
    """è¡Œç‚ºåèª¤æª”æ¡ˆ"""
    overconfidence_score: float  # [0,1]
    regret_aversion_score: float
    mental_accounting_score: float
    framing_effect_score: float
    financial_literacy_score: float
    
    def has_severe_biases(self) -> bool:
        """åš´é‡åèª¤è­¦ç¤º"""
        return (
            self.overconfidence_score > 0.7 or
            self.framing_effect_score > 0.7 or
            (self.overconfidence_score > 0.6 and self.financial_literacy_score < 0.4)
        )
    
    def bias_adjustment(self) -> float:
        """è¡Œç‚ºåèª¤å°Î³çš„èª¿æ•´"""
        # éåº¦è‡ªä¿¡ â†’ Î³ â†“ (éåº¦ç©æ¥µ)
        overconf_adj = -0.15 * max(0, self.overconfidence_score - 0.5)
        
        # å¾Œæ‚”è¶¨é¿ â†’ Î³ â†‘ (æ›´ä¿å®ˆ)
        regret_adj = 0.12 * self.regret_aversion_score
        
        # å¿ƒç†è³¬æˆ¶ â†’ Î³ â†‘ (éç†æ€§ä¿å®ˆ)
        mental_adj = 0.10 * self.mental_accounting_score
        
        # æ¡†æ¶æ•ˆæ‡‰ â†’ Î³ â†‘ (ä¸ä¸€è‡´æ‡²ç½°)
        framing_adj = 0.15 * self.framing_effect_score
        
        return overconf_adj + regret_adj + mental_adj + framing_adj

@dataclass
class RiskProfile:
    """é¢¨éšªåå¥½æª”æ¡ˆï¼ˆæ•´åˆè¡Œç‚ºé‡‘èï¼‰"""
    # åŸºç¤åƒæ•¸
    raw_theta: float
    risk_aversion_gamma: float
    time_horizon_years: float
    
    # è¡Œç‚ºåèª¤
    behavioral_profile: Optional[BehavioralProfile]
    
    # åå¥½
    dividend_preference: float
    loss_tolerance: float
    
    def effective_gamma(self) -> float:
        """æœ‰æ•ˆÎ³ï¼ˆè¡Œç‚ºåèª¤èª¿æ•´ï¼‰"""
        gamma_base = self.risk_aversion_gamma
        
        if self.behavioral_profile:
            behavior_adj = self.behavioral_profile.bias_adjustment()
            gamma_adjusted = gamma_base + behavior_adj
            return np.clip(gamma_adjusted, 0.5, 12.0)
        
        return gamma_base
    
    def gamma_category(self) -> str:
        """Î³åˆ†é¡"""
        gamma = self.effective_gamma()
        if gamma < 1.5:
            return "æ¥µåº¦ç©æ¥µ(logæ•ˆç”¨)"
        elif gamma < 2.5:
            return "ç©æ¥µ(å¯¦è­‰ä¸­ä½æ•¸)"
        elif gamma < 4.0:
            return "ç©©å¥"
        elif gamma < 7.0:
            return "ä¿å®ˆ"
        else:
            return "æ¥µåº¦ä¿å®ˆ"

@dataclass  
class DividendInfo:
    """é…æ¯è³‡è¨Š"""
    latest_date: Optional[datetime]
    latest_amount: float
    ttm_dividend: float
    ttm_yield: float
    data_source: str

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# UI Configuration
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

st.set_page_config(page_title="å°ç£ ETF å€‹äººåŒ–æ¨è–¦ç³»çµ±", layout="wide")
st.title("ğŸ“Š å°ç£ ETF å€‹äººåŒ–æ¨è–¦ç³»çµ±ï¼ˆçµ‚æ¥µå­¸è¡“é‡æ§‹ç‰ˆï¼‰")
st.caption("âš ï¸ åŸºæ–¼å®Œæ•´é¢¨éšªå¿ƒç†çµæ§‹å»ºæ¨¡ + è¡Œç‚ºé‡‘èå­¸ + åš´è¬¹çµ±è¨ˆæ¨è«–")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Sidebar: å®Œæ•´é¢¨éšªåå¥½è©•ä¼°ï¼ˆ14é¡Œå•å·ï¼‰
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

st.sidebar.header("ğŸ‘¤ æŠ•è³‡äººé¢¨éšªå¿ƒç†çµæ§‹è©•ä¼°")
st.sidebar.markdown("**14é¡Œå®Œæ•´å•å·ï¼ˆå­¸è¡“é©—è­‰ï¼‰**")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ç¬¬ä¸€éƒ¨åˆ†ï¼šåŸºç¤é¢¨éšªå®¹å¿åº¦ (6é¡Œ)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

with st.sidebar.expander("ğŸ“‹ ç¬¬ä¸€éƒ¨åˆ†ï¼šåŸºç¤é¢¨éšªå®¹å¿åº¦ (6é¡Œ)", expanded=True):
    st.markdown("**è©•ä¼°åŸºæœ¬é¢¨éšªæ‰¿å—èƒ½åŠ›**")
    
    # Q1: Horizon
    st.markdown("---")
    st.markdown("**Q1. æŠ•è³‡æ™‚é–“ç¯„åœ**")
    horizon_mapping = {
        "å°‘æ–¼ 1 å¹´": (0.5, 0.0),
        "1-3 å¹´": (2, 0.20),
        "4-6 å¹´": (5, 0.40),
        "7-10 å¹´": (8.5, 0.65),
        "10 å¹´ä»¥ä¸Š": (15, 1.0)
    }
    horizon_choice = st.radio(
        "æŠ•è³‡æœŸé–“",
        list(horizon_mapping.keys()),
        index=3,
        key="q1_horizon"
    )
    horizon_years, horizon_score = horizon_mapping[horizon_choice]
    
    # Q2: Capacity
    st.markdown("---")
    st.markdown("**Q2. è³‡é‡‘é…ç½®åå¥½**")
    risk_capacity_mapping = {
        "å­˜å…¥éŠ€è¡Œæˆ–è³¼è²·æ”¿åºœå…¬å‚µ": 0.0,
        "è³¼è²·å‚µåˆ¸å‹åŸºé‡‘": 0.25,
        "è³¼è²·æ··åˆå‹åŸºé‡‘": 0.50,
        "è³¼è²·è‚¡ç¥¨å‹åŸºé‡‘": 0.75,
        "è³¼è²·å€‹è‚¡æˆ–é«˜é¢¨éšªå•†å“": 1.0
    }
    risk_capacity = st.radio(
        "è‹¥æœ‰é–’ç½®è³‡é‡‘",
        list(risk_capacity_mapping.keys()),
        index=2,
        key="q2_capacity"
    )
    capacity_score = risk_capacity_mapping[risk_capacity]
    
    # Q3: Loss Tolerance
    st.markdown("---")
    st.markdown("**Q3. æå¤±å®¹å¿åº¦**")
    loss_tolerance_mapping = {
        "ç«‹å³å…¨éƒ¨è³£å‡º": 0.0,
        "è³£å‡ºä¸€åŠ": 0.20,
        "ç¶­æŒä¸å‹•": 0.50,
        "å°å¹…åŠ ç¢¼": 0.80,
        "å¤§å¹…åŠ ç¢¼": 1.0
    }
    loss_tolerance = st.radio(
        "æŠ•è³‡çµ„åˆä¸€å€‹æœˆå…§ä¸‹è·Œ20%",
        list(loss_tolerance_mapping.keys()),
        index=2,
        key="q3_loss"
    )
    loss_score = loss_tolerance_mapping[loss_tolerance]
    
    # Q4: Income
    st.markdown("---")
    st.markdown("**Q4. æ”¶å…¥ç©©å®šæ€§**")
    income_stability_mapping = {
        "éå¸¸ä¸ç©©å®š": 0.0,
        "ä¸ç©©å®š": 0.25,
        "æ™®é€š": 0.50,
        "ç©©å®š": 0.75,
        "éå¸¸ç©©å®š": 1.0
    }
    income_stability = st.selectbox(
        "æ”¶å…¥ç‹€æ³",
        list(income_stability_mapping.keys()),
        index=2,
        key="q4_income"
    )
    income_score = income_stability_mapping[income_stability]
    
    # Q5: Dividend
    st.markdown("---")
    st.markdown("**Q5. é…æ¯åå¥½**")
    dividend_pref_mapping = {
        "å®Œå…¨ä¸åœ¨ä¹é…æ¯": 0.0,
        "é…æ¯æ¬¡è¦": 0.25,
        "é…æ¯èˆ‡å ±é…¬åŒç­‰é‡è¦": 0.50,
        "é…æ¯å„ªå…ˆ": 0.75,
        "åªè¦ç©©å®šé…æ¯": 1.0
    }
    dividend_pref = st.radio(
        "é…æ¯é‡è¦æ€§",
        list(dividend_pref_mapping.keys()),
        index=2,
        key="q5_dividend"
    )
    dividend_pref_score = dividend_pref_mapping[dividend_pref]
    
    # Q6: Age
    st.markdown("---")
    st.markdown("**Q6. å¹´é½¡**")
    age = st.slider("å¹´é½¡", 20, 80, 35, key="q6_age")
    age_score = max(0, min(1, (80 - age) / 60))

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ç¬¬äºŒéƒ¨åˆ†ï¼šè¡Œç‚ºåèª¤æª¢æ¸¬ (4é¡Œ)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

with st.sidebar.expander("ğŸ§  ç¬¬äºŒéƒ¨åˆ†ï¼šè¡Œç‚ºåèª¤æª¢æ¸¬ (4é¡Œ)", expanded=False):
    st.markdown("**è©•ä¼°æŠ•è³‡å¿ƒç†åèª¤**")
    st.caption("Kahneman & Tversky (1979), Odean (1998)")
    
    # Q7: Overconfidence
    st.markdown("---")
    st.markdown("**Q7. æŠ•è³‡èƒ½åŠ›è‡ªè©•**")
    overconfidence_mapping = {
        "é ä½æ–¼å¹³å‡": 0.0,
        "ç•¥ä½æ–¼å¹³å‡": 0.25,
        "å¹³å‡æ°´å¹³": 0.50,
        "ç•¥é«˜æ–¼å¹³å‡": 0.75,
        "é é«˜æ–¼å¹³å‡": 1.0
    }
    overconfidence = st.radio(
        "æ‚¨èªç‚ºè‡ªå·±çš„æŠ•è³‡èƒ½åŠ›",
        list(overconfidence_mapping.keys()),
        index=2,
        key="q7_overconf"
    )
    overconfidence_score = overconfidence_mapping[overconfidence]
    
    # Q8: Regret
    st.markdown("---")
    st.markdown("**Q8. å¾Œæ‚”å‚¾å‘**")
    regret_mapping = {
        "å¾ä¸å¾Œæ‚”": 0.0,
        "å¶çˆ¾å¾Œæ‚”": 0.25,
        "ç¶“å¸¸å¾Œæ‚”": 0.50,
        "ç¸½æ˜¯å¾Œæ‚”": 0.75,
        "æ¥µåº¦å¾Œæ‚”": 1.0
    }
    regret = st.radio(
        "æŠ•è³‡å¤±èª¤å¾Œ",
        list(regret_mapping.keys()),
        index=2,
        key="q8_regret"
    )
    regret_score = regret_mapping[regret]
    
    # Q9: Mental Accounting
    st.markdown("---")
    st.markdown("**Q9. è³‡é‡‘ä¾†æºæ•æ„Ÿåº¦**")
    mental_acc_mapping = {
        "å®Œå…¨ä¸åœ¨æ„": 0.0,
        "ç¨å¾®åœ¨æ„": 0.25,
        "æœ‰é»åœ¨æ„": 0.50,
        "å¾ˆåœ¨æ„": 0.75,
        "å¼·çƒˆåœ¨æ„": 1.0
    }
    mental_acc = st.radio(
        "å·¥ä½œæ”¶å…¥ vs æŠ•è³‡ç²åˆ©",
        list(mental_acc_mapping.keys()),
        index=2,
        key="q9_mental"
    )
    mental_acc_score = mental_acc_mapping[mental_acc]
    
    # Q10: Framing
    st.markdown("---")
    st.markdown("**Q10. æ¡†æ¶æ•ˆæ‡‰æ¸¬è©¦**")
    framing_q1 = st.radio(
        "æƒ…å¢ƒAï¼šç¢ºå®šç²å¾—30è¬ vs 80%æ©Ÿæœƒç²å¾—40è¬",
        ["é¸æ“‡ç¢ºå®š30è¬", "é¸æ“‡80%æ©Ÿæœƒ40è¬"],
        index=0,
        key="q10_frame1"
    )
    framing_q2 = st.radio(
        "æƒ…å¢ƒBï¼šç¢ºå®šæå¤±30è¬ vs 80%æ©Ÿæœƒæå¤±40è¬",
        ["é¸æ“‡ç¢ºå®šæå¤±30è¬", "é¸æ“‡80%æ©Ÿæœƒæå¤±40è¬"],
        index=1,
        key="q10_frame2"
    )
    # ä¸€è‡´æ€§æª¢æŸ¥
    framing_consistent = (framing_q1 == "é¸æ“‡ç¢ºå®š30è¬" and framing_q2 == "é¸æ“‡ç¢ºå®šæå¤±30è¬") or \
                        (framing_q1 == "é¸æ“‡80%æ©Ÿæœƒ40è¬" and framing_q2 == "é¸æ“‡80%æ©Ÿæœƒæå¤±40è¬")
    framing_score = 0.0 if framing_consistent else 1.0

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ç¬¬ä¸‰éƒ¨åˆ†ï¼šé‡‘èç´ é¤Šè©•ä¼° (4é¡Œ)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

with st.sidebar.expander("ğŸ“š ç¬¬ä¸‰éƒ¨åˆ†ï¼šé‡‘èç´ é¤Šè©•ä¼° (4é¡Œ)", expanded=False):
    st.markdown("**è©•ä¼°é‡‘èçŸ¥è­˜æ°´å¹³**")
    st.caption("Lusardi & Mitchell (2014)")
    
    # Q11: è¤‡åˆ©
    st.markdown("---")
    st.markdown("**Q11. è¤‡åˆ©è¨ˆç®—**")
    q11_answer = st.radio(
        "éŠ€è¡Œåˆ©ç‡2%/å¹´ï¼Œå­˜å…¥100å…ƒï¼Œ5å¹´å¾Œç´„æœ‰ï¼Ÿ",
        ["102å…ƒ", "110å…ƒ", "110.4å…ƒ", "ä¸çŸ¥é“"],
        index=3,
        key="q11_compound"
    )
    q11_correct = (q11_answer == "110.4å…ƒ")
    
    # Q12: é€šè†¨
    st.markdown("---")
    st.markdown("**Q12. é€šè†¨å½±éŸ¿**")
    q12_answer = st.radio(
        "é€šè†¨ç‡2%ï¼Œå­˜æ¬¾åˆ©ç‡1%ï¼Œä¸€å¹´å¾Œè³¼è²·åŠ›ï¼Ÿ",
        ["å¢åŠ ", "ä¸è®Š", "é™ä½", "ä¸çŸ¥é“"],
        index=3,
        key="q12_inflation"
    )
    q12_correct = (q12_answer == "é™ä½")
    
    # Q13: åˆ†æ•£
    st.markdown("---")
    st.markdown("**Q13. åˆ†æ•£æŠ•è³‡**")
    q13_answer = st.radio(
        "ä½•è€…é¢¨éšªé€šå¸¸è¼ƒä½ï¼Ÿ",
        ["å–®ä¸€å…¬å¸è‚¡ç¥¨", "è‚¡ç¥¨å‹åŸºé‡‘", "é¢¨éšªç›¸åŒ", "ä¸çŸ¥é“"],
        index=3,
        key="q13_diversify"
    )
    q13_correct = (q13_answer == "è‚¡ç¥¨å‹åŸºé‡‘")
    
    # Q14: Sharpe
    st.markdown("---")
    st.markdown("**Q14. é¢¨éšªèª¿æ•´å ±é…¬**")
    q14_answer = st.radio(
        "A: å ±é…¬15%/é¢¨éšª20% vs B: å ±é…¬12%/é¢¨éšª10%",
        ["Aè¼ƒå„ª", "Bè¼ƒå„ª", "ç›¸åŒ", "ä¸çŸ¥é“"],
        index=3,
        key="q14_sharpe"
    )
    q14_correct = (q14_answer == "Bè¼ƒå„ª")
    
    financial_literacy_score = sum([q11_correct, q12_correct, q13_correct, q14_correct]) / 4.0

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ä¸€è‡´æ€§æª¢é©—
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

inconsistencies = []

if horizon_score > 0.6 and loss_score < 0.3:
    inconsistencies.append("âš ï¸ çŸ›ç›¾ï¼šé•·æœŸæŠ•è³‡ä½†æå¤±å®¹å¿åº¦ä½")

if capacity_score > 0.7 and loss_score < 0.3:
    inconsistencies.append("âš ï¸ çŸ›ç›¾ï¼šé¡˜æ„è²·é«˜é¢¨éšªä½†è·Œ20%å°±è³£")

if overconfidence_score > 0.6 and financial_literacy_score < 0.5:
    inconsistencies.append("âš ï¸ å±éšªï¼šéåº¦è‡ªä¿¡ä½†é‡‘èçŸ¥è­˜ä¸è¶³")

if framing_score > 0.5:
    inconsistencies.append("âš ï¸ æ¡†æ¶æ•ˆæ‡‰ï¼šç²åˆ©ä¿å®ˆ+è™§æå†’éšª")

if inconsistencies:
    with st.sidebar.expander("âš ï¸ ä¸€è‡´æ€§æª¢é©—è­¦ç¤º", expanded=True):
        for issue in inconsistencies:
            st.warning(issue)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Î¸ è¨ˆç®—ï¼ˆæ•´åˆè¡Œç‚ºåèª¤ï¼‰
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

st.sidebar.markdown("---")
st.sidebar.subheader("ğŸ“Š é¢¨éšªåå¥½è¨ˆç®—ï¼ˆæ•´åˆç‰ˆï¼‰")

# åŸºç¤Î¸
EMPIRICAL_WEIGHTS_BASE = {
    'capacity': 0.30,
    'loss': 0.30,
    'horizon': 0.20,
    'income': 0.10,
    'age': 0.10
}

theta_base = (
    EMPIRICAL_WEIGHTS_BASE['capacity'] * capacity_score +
    EMPIRICAL_WEIGHTS_BASE['loss'] * loss_score +
    EMPIRICAL_WEIGHTS_BASE['horizon'] * horizon_score +
    EMPIRICAL_WEIGHTS_BASE['income'] * income_score +
    EMPIRICAL_WEIGHTS_BASE['age'] * age_score
)

# å‰µå»ºè¡Œç‚ºæª”æ¡ˆ
behavioral_profile = BehavioralProfile(
    overconfidence_score=overconfidence_score,
    regret_aversion_score=regret_score,
    mental_accounting_score=mental_acc_score,
    framing_effect_score=framing_score,
    financial_literacy_score=financial_literacy_score
)

# è¡Œç‚ºåèª¤èª¿æ•´
behavior_adjustment = behavioral_profile.bias_adjustment()

# é‡‘èç´ é¤Šèª¿æ•´
if financial_literacy_score >= 0.75:
    literacy_adjustment = -0.10 * (theta_base - 0.5)
elif financial_literacy_score <= 0.25:
    literacy_adjustment = 0.15
else:
    literacy_adjustment = 0.05

# ä¸€è‡´æ€§æ‡²ç½°
consistency_penalty = len(inconsistencies) * 0.05

# æœ€çµ‚Î¸
theta_adjusted = theta_base + (behavior_adjustment / 3.0) + (literacy_adjustment / 2.0) + consistency_penalty
theta = np.clip(theta_adjusted, 0, 1)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Î³ æ˜ å°„ï¼ˆå‹•æ…‹ç¯„åœ - åŸºæ–¼é‡‘èç´ é¤Šï¼‰
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def theta_to_gamma_dynamic(theta: float, financial_literacy: float) -> float:
    """
    å‹•æ…‹Î³æ˜ å°„ï¼ˆæ•´åˆé‡‘èç´ é¤Šï¼‰
    
    ç†è«–ï¼š
    - é«˜ç´ é¤Šï¼šÎ³ âˆˆ [1.0, 4.0]ï¼ˆç†æ€§ç¯„åœï¼ŒChetty 2006ï¼‰
    - ä¸­ç´ é¤Šï¼šÎ³ âˆˆ [0.5, 7.0]ï¼ˆå…è¨±åå·®ï¼‰
    - ä½ç´ é¤Šï¼šÎ³ âˆˆ [0.5, 10.0]ï¼ˆè¡Œç‚ºåèª¤å¤§ï¼‰
    """
    if financial_literacy >= 0.75:
        # é«˜ç´ é¤Šï¼šç†æ€§çª„ç¯„åœ
        gamma_min, gamma_max = 1.0, 4.0
        power = 1.5
    elif financial_literacy >= 0.50:
        # ä¸­ç´ é¤Šï¼šä¸­ç­‰ç¯„åœ
        gamma_min, gamma_max = 0.5, 7.0
        power = 1.8
    else:
        # ä½ç´ é¤Šï¼šå¯¬ç¯„åœï¼ˆå¯èƒ½æ¥µç«¯ï¼‰
        gamma_min, gamma_max = 0.5, 10.0
        power = 2.0
    
    gamma = gamma_min + (gamma_max - gamma_min) * ((1 - theta) ** power)
    return gamma

risk_aversion_gamma = theta_to_gamma_dynamic(theta, financial_literacy_score)

# å‰µå»ºé¢¨éšªæª”æ¡ˆ
risk_profile = RiskProfile(
    raw_theta=theta,
    risk_aversion_gamma=risk_aversion_gamma,
    time_horizon_years=horizon_years,
    behavioral_profile=behavioral_profile,
    dividend_preference=dividend_pref_score,
    loss_tolerance=loss_score
)

effective_gamma = risk_profile.effective_gamma()

# é¡¯ç¤ºçµæœ
st.sidebar.metric("Î¸_base", f"{theta_base:.3f}")
st.sidebar.metric("Î¸_final", f"{theta:.3f}", delta=f"{theta - theta_base:+.3f}")
st.sidebar.metric("Î³ (åŸºç¤)", f"{risk_aversion_gamma:.2f}")
st.sidebar.metric("Î³_eff (è¡Œç‚ºèª¿æ•´)", f"{effective_gamma:.2f}", delta=f"{effective_gamma - risk_aversion_gamma:+.2f}")
st.sidebar.metric("é‡‘èç´ é¤Š", f"{financial_literacy_score:.0%}")

gamma_category = risk_profile.gamma_category()
st.sidebar.info(f"**é¢¨éšªé¡å‹**ï¼š{gamma_category}")

if behavioral_profile.has_severe_biases():
    st.sidebar.error("âš ï¸ åµæ¸¬åˆ°åš´é‡è¡Œç‚ºåèª¤ï¼å»ºè­°è«®è©¢å°ˆæ¥­é¡§å•")

# è©•åˆ†æ˜ç´°
with st.sidebar.expander("ğŸ“‹ è©³ç´°è©•åˆ†æ˜ç´°"):
    st.write("**ç¬¬ä¸€éƒ¨åˆ†ï¼šåŸºç¤Î¸**")
    st.write(f"- é¢¨éšªæ‰¿å—ï¼š{capacity_score:.2f} Ã— {EMPIRICAL_WEIGHTS_BASE['capacity']:.0%} = {capacity_score * EMPIRICAL_WEIGHTS_BASE['capacity']:.3f}")
    st.write(f"- æå¤±å®¹å¿ï¼š{loss_score:.2f} Ã— {EMPIRICAL_WEIGHTS_BASE['loss']:.0%} = {loss_score * EMPIRICAL_WEIGHTS_BASE['loss']:.3f}")
    st.write(f"- æŠ•è³‡æœŸé–“ï¼š{horizon_score:.2f} Ã— {EMPIRICAL_WEIGHTS_BASE['horizon']:.0%} = {horizon_score * EMPIRICAL_WEIGHTS_BASE['horizon']:.3f}")
    st.write(f"**Î¸_base = {theta_base:.3f}**")
    
    st.divider()
    st.write("**ç¬¬äºŒéƒ¨åˆ†ï¼šè¡Œç‚ºåèª¤**")
    st.write(f"- éåº¦è‡ªä¿¡ï¼š{overconfidence_score:.2f} â†’ {-0.15 * max(0, overconfidence_score - 0.5):+.3f}")
    st.write(f"- å¾Œæ‚”è¶¨é¿ï¼š{regret_score:.2f} â†’ {0.12 * regret_score:+.3f}")
    st.write(f"- å¿ƒç†è³¬æˆ¶ï¼š{mental_acc_score:.2f} â†’ {0.10 * mental_acc_score:+.3f}")
    st.write(f"- æ¡†æ¶æ•ˆæ‡‰ï¼š{framing_score:.2f} â†’ {0.15 * framing_score:+.3f}")
    st.write(f"**è¡Œç‚ºèª¿æ•´ = {behavior_adjustment:+.3f}**")
    
    st.divider()
    st.write("**ç¬¬ä¸‰éƒ¨åˆ†ï¼šé‡‘èç´ é¤Š**")
    st.write(f"- ç´ é¤Šåˆ†æ•¸ï¼š{financial_literacy_score:.0%}")
    st.write(f"- Î³ç¯„åœï¼š{'[1.0, 4.0]ç†æ€§' if financial_literacy_score >= 0.75 else '[0.5, 10.0]å¯¬ç¯„åœ'}")
    st.write(f"**Î³ = {risk_aversion_gamma:.2f}**")
    st.write(f"**Î³_eff (è¡Œç‚ºèª¿æ•´) = {effective_gamma:.2f}**")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# æ’åºé¸é …
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

st.sidebar.markdown("---")
st.sidebar.header("ğŸ“Š æ’åºé¸æ“‡")
sort_option = st.sidebar.selectbox(
    "æ’åºä¾æ“š", 
    [
        "æ•ˆç”¨åˆ†æ•¸ï¼ˆCRRAæ­£ç¢ºç‰ˆï¼‰",
        "Sharpe Ratio",
        "Sortino Ratio",
        "å¹´åŒ–å ±é…¬ç‡",
        "TTM æ®–åˆ©ç‡"
    ]
)

st.sidebar.header("ğŸ“ˆ Top N ETF")
TOP_N = st.sidebar.slider("é¡¯ç¤ºæ•¸é‡", 1, len(ETF_LIST), 5)

st.sidebar.markdown("---")
if st.sidebar.button("ğŸ”„ é‡æ–°è¨ˆç®—"):
    st.cache_data.clear()
    st.sidebar.success("âœ… å¿«å–å·²æ¸…é™¤")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# çµ±è¨ˆå·¥å…·
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class StatisticalTools:
    @staticmethod
    def newey_west_se(residuals: np.ndarray, X: np.ndarray, lags: int = NEWEY_WEST_LAGS) -> np.ndarray:
        """Newey-West HACæ¨™æº–èª¤"""
        n, k = X.shape
        XtX_inv = np.linalg.inv(X.T @ X)
        
        omega = np.zeros((k, k))
        for lag in range(lags + 1):
            weight = 1 - lag / (lags + 1) if lag > 0 else 1
            
            for i in range(lag, n):
                outer_prod = np.outer(X[i] * residuals[i], X[i-lag] * residuals[i-lag])
                omega += weight * outer_prod
                if lag > 0:
                    omega += weight * outer_prod.T
        
        variance = XtX_inv @ omega @ XtX_inv
        return np.sqrt(np.diag(variance))
    
    @staticmethod
    def bootstrap_sharpe_ci(returns: np.ndarray, n_iterations: int = BOOTSTRAP_ITERATIONS) -> Tuple[float, float]:
        """Bootstrap Sharpe CI"""
        sharpe_samples = []
        n = len(returns)
        
        for _ in range(n_iterations):
            sample = np.random.choice(returns, size=n, replace=True)
            mean = sample.mean() * TRADING_DAYS
            std = sample.std() * np.sqrt(TRADING_DAYS)
            sharpe = (mean - RISK_FREE_RATE) / std if std > 0 else 0
            sharpe_samples.append(sharpe)
        
        sharpe_samples = np.array(sharpe_samples)
        lower = np.percentile(sharpe_samples, 2.5)
        upper = np.percentile(sharpe_samples, 97.5)
        
        return lower, upper
    
    @staticmethod
    def jarque_bera_test(returns: np.ndarray) -> float:
        """JBå¸¸æ…‹æ€§æª¢é©—"""
        n = len(returns)
        skew = stats.skew(returns)
        kurt = stats.kurtosis(returns, fisher=True)
        
        jb_stat = (n / 6) * (skew**2 + (kurt**2) / 4)
        pvalue = 1 - stats.chi2.cdf(jb_stat, df=2)
        
        return pvalue

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# è³‡æ–™æŠ“å–
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class DataFetcher:
    @staticmethod
    @st.cache_data(ttl=3600)
    def fetch_price_data(etf_list: Dict, benchmark: str, period: str = "1y") -> Dict:
        data = {}
        tickers = list(set(list(etf_list.keys()) + [benchmark]))
        
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        for i, code in enumerate(tickers):
            try:
                status_text.text(f"è¼‰å…¥ {code}...")
                ticker = yf.Ticker(code)
                df = ticker.history(period=period)
                
                if not df.empty and len(df) >= MIN_TRADING_DAYS:
                    data[code] = df
                else:
                    data[code] = None
                    
            except Exception as e:
                logger.error(f"{code} å¤±æ•—: {e}")
                data[code] = None
            
            progress_bar.progress((i + 1) / len(tickers))
        
        progress_bar.empty()
        status_text.empty()
        
        return data
    
    @staticmethod
    @st.cache_data(ttl=900)
    def fetch_latest_price(code: str) -> Optional[float]:
        try:
            ticker = yf.Ticker(code)
            hist = ticker.history(period="5d")
            if not hist.empty:
                return float(hist['Close'].iloc[-1])
        except:
            pass
        return None
    
    @staticmethod
    @st.cache_data(ttl=7200)
    def fetch_dividend_info(etf_code: str) -> DividendInfo:
        stock_code = etf_code.replace('.TW', '')
        
        try:
            url = "https://api.finmindtrade.com/api/v4/data"
            params = {
                "dataset": "TaiwanStockDividend",
                "data_id": stock_code,
                "start_date": (datetime.now() - timedelta(days=400)).strftime('%Y-%m-%d'),
                "token": ""
            }
            
            response = requests.get(url, params=params, timeout=10)
            
            if response.status_code == 200:
                data = response.json()
                if data.get('status') == 200 and data.get('data'):
                    df = pd.DataFrame(data['data'])
                    
                    if not df.empty:
                        df['date'] = pd.to_datetime(df.get('date', df.get('ex_dividend_date')))
                        df = df.sort_values('date', ascending=False)
                        
                        dividend_col = next((col for col in ['cash_dividend', 'CashDividend'] 
                                           if col in df.columns), None)
                        
                        if dividend_col:
                            df[dividend_col] = pd.to_numeric(df[dividend_col], errors='coerce')
                            df = df[df[dividend_col] > 0].dropna(subset=[dividend_col])
                            
                            if not df.empty:
                                one_year_ago = datetime.now() - timedelta(days=365)
                                ttm_df = df[df['date'] >= one_year_ago]
                                ttm_sum = ttm_df[dividend_col].sum()
                                
                                latest_price = DataFetcher.fetch_latest_price(etf_code) or 100
                                ttm_yield = (ttm_sum / latest_price * 100) if latest_price > 0 else 0
                                
                                return DividendInfo(
                                    latest_date=df.iloc[0]['date'],
                                    latest_amount=float(df.iloc[0][dividend_col]),
                                    ttm_dividend=float(ttm_sum),
                                    ttm_yield=ttm_yield,
                                    data_source="FinMind"
                                )
        except Exception as e:
            logger.warning(f"FinMind å¤±æ•—: {e}")
        
        return DataFetcher._get_static_dividend(stock_code)
    
    @staticmethod
    def _get_static_dividend(stock_code: str) -> DividendInfo:
        static_data = {
            "0050": DividendInfo(datetime(2024, 7, 22), 3.00, 5.50, 3.2, "éœæ…‹"),
            "0056": DividendInfo(datetime(2025, 1, 22), 2.00, 4.20, 6.5, "éœæ…‹"),
            "006208": DividendInfo(datetime(2024, 7, 22), 0.65, 1.30, 2.9, "éœæ…‹"),
            "00878": DividendInfo(datetime(2024, 11, 22), 0.38, 1.52, 7.2, "éœæ…‹"),
            "00919": DividendInfo(datetime(2025, 1, 22), 0.62, 2.32, 9.1, "éœæ…‹"),
            "00692": DividendInfo(datetime(2024, 7, 22), 0.48, 0.96, 3.1, "éœæ…‹"),
            "00757": DividendInfo(datetime(2024, 8, 22), 0.28, 0.56, 2.5, "éœæ…‹"),
        }
        return static_data.get(stock_code, DividendInfo(None, 0.0, 0.0, 0.0, "ç„¡"))

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# è²¡å‹™åˆ†æï¼ˆæ­£ç¢ºCRRAæ•ˆç”¨ï¼‰
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class FinancialAnalyzer:
    @staticmethod
    def calculate_metrics(etf_df: pd.DataFrame, market_df: pd.DataFrame) -> FinancialMetrics:
        r = etf_df["Close"].pct_change().dropna()
        mr = market_df["Close"].pct_change().dropna()
        
        idx = r.index.intersection(mr.index)
        r, mr = r.loc[idx], mr.loc[idx]
        
        n = len(r)
        if n < MIN_TRADING_DAYS:
            raise ValueError("è³‡æ–™ä¸è¶³")
        
        # åŸºæœ¬çµ±è¨ˆ
        ann_return = r.mean() * TRADING_DAYS
        ann_volatility = r.std() * np.sqrt(TRADING_DAYS)
        
        # é«˜éšå‹•å·®
        skewness = stats.skew(r)
        excess_kurtosis = stats.kurtosis(r, fisher=True)
        
        # CAPM
        from sklearn.linear_model import LinearRegression
        X = mr.values.reshape(-1, 1)
        y = r.values
        
        model = LinearRegression()
        model.fit(X, y)
        beta = model.coef_[0]
        alpha = model.intercept_ * TRADING_DAYS
        
        # Residuals for HAC
        residuals = y - model.predict(X)
        X_with_const = np.column_stack([np.ones(len(X)), X])
        se_hac = StatisticalTools.newey_west_se(residuals, X_with_const)
        
        # Sharpe
        sharpe = (ann_return - RISK_FREE_RATE) / ann_volatility if ann_volatility > 0 else 0
        sharpe_se_hac = np.sqrt((1 + 0.5 * sharpe**2) / n)
        sharpe_pvalue = 2 * (1 - stats.t.cdf(abs(sharpe / sharpe_se_hac), n-1))
        
        # Bootstrap CI
        sharpe_ci_lower, sharpe_ci_upper = StatisticalTools.bootstrap_sharpe_ci(r.values)
        
        # Sortino
        downside_returns = r[r < 0]
        downside_std = downside_returns.std() * np.sqrt(TRADING_DAYS) if len(downside_returns) > 0 else ann_volatility
        sortino = (ann_return - RISK_FREE_RATE) / downside_std if downside_std > 0 else 0
        
        # Max Drawdown
        cumulative = (1 + r).cumprod()
        running_max = cumulative.expanding().max()
        drawdown = (cumulative - running_max) / running_max
        max_drawdown = drawdown.min()
        
        # JB test
        jb_pvalue = StatisticalTools.jarque_bera_test(r.values)
        
        return FinancialMetrics(
            ann_return=ann_return,
            ann_volatility=ann_volatility,
            sharpe_ratio=sharpe,
            sortino_ratio=sortino,
            beta=beta,
            alpha=alpha,
            max_drawdown=max_drawdown,
            skewness=skewness,
            excess_kurtosis=excess_kurtosis,
            return_se_hac=se_hac[0] * TRADING_DAYS,
            sharpe_se_hac=sharpe_se_hac,
            sharpe_ci_lower=sharpe_ci_lower,
            sharpe_ci_upper=sharpe_ci_upper,
            sharpe_pvalue=sharpe_pvalue,
            jb_pvalue=jb_pvalue,
            n_observations=n
        )
    
    @staticmethod
    def calculate_utility_crra_correct(metrics: FinancialMetrics,
                                      risk_profile: RiskProfile,
                                      dividend_yield: float,
                                      current_wealth: float = 1000000) -> float:
        """
        æ­£ç¢ºCRRAæ•ˆç”¨ï¼ˆTaylorå±•é–‹å«é«˜éšå‹•å·®ï¼‰
        
        U(W) = W^(1-Î³)/(1-Î³)
        E[U] â‰ˆ U(E[W]) + 0.5Â·U''Â·Var + (1/6)Â·U'''Â·Skew + (1/24)Â·U''''Â·Kurt
        """
        gamma = risk_profile.effective_gamma()
        
        # ç¸½å ±é…¬
        capital_gain = metrics.ann_return
        dividend_contrib = (dividend_yield / 100) * (1 + risk_profile.dividend_preference)
        total_return = capital_gain + dividend_contrib
        
        # æœŸæœ›è²¡å¯Œ
        expected_wealth = current_wealth * (1 + total_return)
        
        # è²¡å¯Œçš„å‹•å·®
        sigma = metrics.ann_volatility
        variance_wealth = (current_wealth * sigma) ** 2
        skew_wealth = current_wealth**3 * (sigma**3) * metrics.skewness
        kurt_wealth = current_wealth**4 * (sigma**4) * (metrics.excess_kurtosis + 3)
        
        # CRRAå°æ•¸
        if abs(gamma - 1.0) < 1e-6:
            u_prime = 1 / expected_wealth
            u_double_prime = -1 / (expected_wealth ** 2)
            u_triple_prime = 2 / (expected_wealth ** 3)
            u_quad_prime = -6 / (expected_wealth ** 4)
        else:
            u_prime = expected_wealth ** (-gamma)
            u_double_prime = -gamma * (expected_wealth ** (-gamma - 1))
            u_triple_prime = gamma * (gamma + 1) * (expected_wealth ** (-gamma - 2))
            u_quad_prime = -gamma * (gamma + 1) * (gamma + 2) * (expected_wealth ** (-gamma - 3))
        
        # Taylorå±•é–‹
        utility = (
            expected_wealth ** (1 - gamma) / (1 - gamma) if abs(gamma - 1.0) > 1e-6 else np.log(expected_wealth)
        ) + (
            0.5 * u_double_prime * variance_wealth +
            (1/6) * u_triple_prime * skew_wealth +
            (1/24) * u_quad_prime * kurt_wealth
        )
        
        # æ¨™æº–åŒ–
        utility_normalized = utility / (current_wealth ** (1 - gamma)) if abs(gamma - 1.0) > 1e-6 else utility / np.log(current_wealth)
        
        return utility_normalized

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ä¸»æµç¨‹
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

with st.spinner("è¼‰å…¥è³‡æ–™..."):
    fetcher = DataFetcher()
    analyzer = FinancialAnalyzer()
    
    price_data = fetcher.fetch_price_data(ETF_LIST, MARKET_BENCHMARK)
    market_df = price_data.get(MARKET_BENCHMARK)

if market_df is None:
    st.error("âŒ ç„¡æ³•è¼‰å…¥å¸‚å ´åŸºæº–")
    st.stop()

# è¨ˆç®—æ‰€æœ‰ETF
results = []
sharpe_pvalues = []

for etf_code, etf_info in ETF_LIST.items():
    etf_df = price_data.get(etf_code)
    
    if etf_df is None or len(etf_df) < MIN_TRADING_DAYS:
        continue
    
    try:
        metrics = analyzer.calculate_metrics(etf_df, market_df)
        div_info = fetcher.fetch_dividend_info(etf_code)
        latest_price = fetcher.fetch_latest_price(etf_code) or float(etf_df["Close"].iloc[-1])
        
        utility_score = analyzer.calculate_utility_crra_correct(
            metrics, risk_profile, div_info.ttm_yield
        )
        
        sharpe_pvalues.append(metrics.sharpe_pvalue)
        
        result = {
            "ETF": etf_code,
            "åç¨±": etf_info["name"],
            "é¡å‹": etf_info["type"],
            "æœ€æ–°åƒ¹": round(latest_price, 2),
            "TTMæ®–åˆ©ç‡%": div_info.ttm_yield,
            
            "å¹´åŒ–å ±é…¬%": round(metrics.ann_return * 100, 2),
            "å¹´åŒ–æ³¢å‹•%": round(metrics.ann_volatility * 100, 2),
            "Sharpe Ratio": round(metrics.sharpe_ratio, 3),
            "Sortino Ratio": round(metrics.sortino_ratio, 3),
            "Beta": round(metrics.beta, 3),
            "æœ€å¤§å›æ’¤%": round(metrics.max_drawdown * 100, 2),
            
            "ååº¦": round(metrics.skewness, 3),
            "è¶…é¡å³°åº¦": round(metrics.excess_kurtosis, 3),
            
            "æ•ˆç”¨åˆ†æ•¸": round(utility_score, 6),
            
            "Sharpe på€¼": round(metrics.sharpe_pvalue, 4),
            "JB på€¼": round(metrics.jb_pvalue, 4),
            "å¸¸æ…‹æ€§": "âœ“" if metrics.jb_pvalue > 0.05 else "âœ—",
        }
        
        results.append(result)
        
    except Exception as e:
        logger.error(f"{etf_code} å¤±æ•—: {e}")
        continue

df_results = pd.DataFrame(results)

if df_results.empty:
    st.error("âŒ ç„¡è³‡æ–™")
    st.stop()

# å¤šé‡æª¢é©—æ ¡æ­£
if sharpe_pvalues:
    from statsmodels.stats.multitest import multipletests
    reject, pvals_corrected, _, _ = multipletests(sharpe_pvalues, alpha=0.05, method='holm')
    
    df_results["Sharpeé¡¯è‘—(æ ¡æ­£)"] = ["âœ“" if r else "âœ—" for r in reject]

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# æ’åº
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

sort_mapping = {
    "æ•ˆç”¨åˆ†æ•¸ï¼ˆCRRAæ­£ç¢ºç‰ˆï¼‰": ("æ•ˆç”¨åˆ†æ•¸", False),
    "Sharpe Ratio": ("Sharpe Ratio", False),
    "Sortino Ratio": ("Sortino Ratio", False),
    "å¹´åŒ–å ±é…¬ç‡": ("å¹´åŒ–å ±é…¬%", False),
    "TTM æ®–åˆ©ç‡": ("TTMæ®–åˆ©ç‡%", False)
}

sort_col, sort_asc = sort_mapping[sort_option]
df_sorted = df_results.sort_values(sort_col, ascending=sort_asc)
df_top = df_sorted.head(TOP_N)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# UI é¡¯ç¤º
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

st.subheader(f"ğŸ¯ Top {TOP_N} ETF æ¨è–¦")

col1, col2, col3, col4 = st.columns(4)
with col1:
    st.metric("Î¸ (èª¿æ•´å¾Œ)", f"{theta:.3f}")
with col2:
    st.metric("Î³_eff", f"{effective_gamma:.2f}")
with col3:
    st.metric("é‡‘èç´ é¤Š", f"{financial_literacy_score:.0%}")
with col4:
    st.metric("é¢¨éšªé¡å‹", gamma_category)

st.caption(
    f"**æ’åºä¾æ“š**: {sort_option} | "
    f"**è¡Œç‚ºåèª¤èª¿æ•´**: {behavior_adjustment:+.3f} | "
    f"**ä¸€è‡´æ€§**: {len(inconsistencies)}å€‹çŸ›ç›¾"
)

# ä¸»è¡¨æ ¼
display_cols = [
    "ETF", "åç¨±", "é¡å‹", "æœ€æ–°åƒ¹", "TTMæ®–åˆ©ç‡%",
    "å¹´åŒ–å ±é…¬%", "å¹´åŒ–æ³¢å‹•%", "Sharpe Ratio", "æ•ˆç”¨åˆ†æ•¸",
    "Sharpeé¡¯è‘—(æ ¡æ­£)", "å¸¸æ…‹æ€§"
]

st.dataframe(
    df_top[display_cols].style.format({
        "æœ€æ–°åƒ¹": "{:.2f}",
        "TTMæ®–åˆ©ç‡%": "{:.2f}%",
        "å¹´åŒ–å ±é…¬%": "{:.2f}%",
        "å¹´åŒ–æ³¢å‹•%": "{:.2f}%",
        "Sharpe Ratio": "{:.3f}",
        "æ•ˆç”¨åˆ†æ•¸": "{:.6f}",
    }),
    use_container_width=True
)

# çµ±è¨ˆæ‘˜è¦
col1, col2, col3 = st.columns(3)
with col1:
    avg_utility = df_top["æ•ˆç”¨åˆ†æ•¸"].mean()
    st.metric("å¹³å‡æ•ˆç”¨", f"{avg_utility:.6f}")
with col2:
    utility_spread = df_top["æ•ˆç”¨åˆ†æ•¸"].max() - df_top["æ•ˆç”¨åˆ†æ•¸"].min()
    st.metric("æ•ˆç”¨å·®ç•°", f"{utility_spread:.6f}")
with col3:
    sig_count = df_top[df_top["Sharpeé¡¯è‘—(æ ¡æ­£)"] == "âœ“"].shape[0]
    st.metric("Sharpeé¡¯è‘—", f"{sig_count}/{TOP_N}")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# è¦–è¦ºåŒ–
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

st.subheader("ğŸ•¸ï¸ å¤šç¶­åº¦ç¸¾æ•ˆé›·é”åœ–")

radar_metrics = ["å¹´åŒ–å ±é…¬%", "Sharpe Ratio", "Sortino Ratio", "TTMæ®–åˆ©ç‡%"]
radar_data = df_top.copy()

for col in radar_metrics:
    min_val = df_results[col].min()
    max_val = df_results[col].max()
    if max_val > min_val:
        radar_data[f"{col}_norm"] = (radar_data[col] - min_val) / (max_val - min_val)
    else:
        radar_data[f"{col}_norm"] = 0.5

fig_radar = go.Figure()

for _, row in radar_data.iterrows():
    values = [row[f"{m}_norm"] for m in radar_metrics]
    values.append(values[0])
    
    fig_radar.add_trace(go.Scatterpolar(
        r=values,
        theta=radar_metrics + [radar_metrics[0]],
        fill='toself',
        name=row["åç¨±"],
        opacity=0.6
    ))

fig_radar.update_layout(
    polar=dict(radialaxis=dict(visible=True, range=[0, 1])),
    showlegend=True
)

st.plotly_chart(fig_radar, use_container_width=True)

# é¢¨éšªå ±é…¬æ•£ä½ˆåœ–
st.subheader("ğŸ’­ é¢¨éšªå ±é…¬æ•£ä½ˆåœ–")

fig_scatter = px.scatter(
    df_results,
    x="å¹´åŒ–æ³¢å‹•%",
    y="å¹´åŒ–å ±é…¬%",
    size="TTMæ®–åˆ©ç‡%",
    color="æ•ˆç”¨åˆ†æ•¸",
    hover_data=["åç¨±", "Sharpe Ratio"],
    color_continuous_scale="RdYlGn"
)

st.plotly_chart(fig_scatter, use_container_width=True)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# å®Œæ•´æ¯”è¼ƒè¡¨
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

st.divider()
st.subheader("ğŸ“Š å®Œæ•´ETFæ¯”è¼ƒè¡¨")

full_cols = [
    "ETF", "åç¨±", "é¡å‹", "å¹´åŒ–å ±é…¬%", "å¹´åŒ–æ³¢å‹•%", "Sharpe Ratio",
    "Beta", "æœ€å¤§å›æ’¤%", "ååº¦", "è¶…é¡å³°åº¦", "æ•ˆç”¨åˆ†æ•¸", "Sharpeé¡¯è‘—(æ ¡æ­£)", "å¸¸æ…‹æ€§"
]

st.dataframe(
    df_results[full_cols].sort_values("æ•ˆç”¨åˆ†æ•¸", ascending=False).style.format({
        "å¹´åŒ–å ±é…¬%": "{:.2f}%",
        "å¹´åŒ–æ³¢å‹•%": "{:.2f}%",
        "Sharpe Ratio": "{:.3f}",
        "Beta": "{:.3f}",
        "æœ€å¤§å›æ’¤%": "{:.2f}%",
        "ååº¦": "{:.3f}",
        "è¶…é¡å³°åº¦": "{:.3f}",
        "æ•ˆç”¨åˆ†æ•¸": "{:.6f}",
    }),
    use_container_width=True
)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# æ•æ„Ÿåº¦åˆ†æ
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

st.divider()
st.subheader("ğŸ”¬ åƒæ•¸æ•æ„Ÿåº¦åˆ†æ")

with st.expander("æŸ¥çœ‹ä¸åŒÎ³å€¼ä¸‹çš„Top 3è®ŠåŒ–"):
    sensitivity_data = []
    
    test_gammas = [0.5, 1.5, 2.5, 4.0, 7.0]
    
    for test_gamma in test_gammas:
        temp_results = []
        
        for _, row in df_results.iterrows():
            etf_code = row["ETF"]
            etf_df = price_data.get(etf_code)
            
            if etf_df is not None:
                metrics = analyzer.calculate_metrics(etf_df, market_df)
                div_info = fetcher.fetch_dividend_info(etf_code)
                
                # è‡¨æ™‚æª”æ¡ˆ
                temp_profile = RiskProfile(
                    raw_theta=theta,
                    risk_aversion_gamma=test_gamma,
                    time_horizon_years=horizon_years,
                    behavioral_profile=None,
                    dividend_preference=dividend_pref_score,
                    loss_tolerance=loss_score
                )
                
                utility = analyzer.calculate_utility_crra_correct(
                    metrics, temp_profile, div_info.ttm_yield
                )
                
                temp_results.append({
                    "Î³": test_gamma,
                    "ETF": etf_code,
                    "æ•ˆç”¨": utility
                })
        
        temp_df = pd.DataFrame(temp_results).sort_values("æ•ˆç”¨", ascending=False).head(3)
        top3 = ", ".join(temp_df["ETF"].str.replace(".TW", "").tolist())
        
        risk_type = (
            "æ¥µåº¦ç©æ¥µ" if test_gamma < 1.5 else
            "ç©æ¥µ" if test_gamma < 2.5 else
            "ç©©å¥" if test_gamma < 4.0 else
            "ä¿å®ˆ"
        )
        
        sensitivity_data.append({
            "Î³": test_gamma,
            "é¢¨éšªé¡å‹": risk_type,
            "Top 3": top3
        })
    
    st.table(pd.DataFrame(sensitivity_data))

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# æ–¹æ³•è«–èªªæ˜
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

st.divider()
st.subheader("ğŸ“– æ–¹æ³•è«–èªªæ˜ï¼ˆçµ‚æ¥µç‰ˆï¼‰")

with st.expander("ğŸ“ çµ‚æ¥µæ”¹é€²ç¸½çµ"):
    st.markdown("""
    ### çµ‚æ¥µç‰ˆæ”¹é€²è¦é»
    
    #### 1. **å•å·é©å‘½** (6é¡Œ â†’ 14é¡Œ)
    
    | ç¶­åº¦ | é¡Œæ•¸ | ç†è«–ä¾æ“š |
    |------|------|---------|
    | åŸºç¤é¢¨éšªå®¹å¿åº¦ | 6é¡Œ | Grable & Lytton (1999) |
    | è¡Œç‚ºåèª¤æª¢æ¸¬ | 4é¡Œ | Kahneman & Tversky (1979) |
    | é‡‘èç´ é¤Šè©•ä¼° | 4é¡Œ | Lusardi & Mitchell (2014) |
    | ä¸€è‡´æ€§æª¢é©— | è‡ªå‹• | Cronbach's Î± |
    
    **æ–°å¢èƒ½åŠ›**ï¼š
    - éåº¦è‡ªä¿¡åµæ¸¬ï¼ˆèƒ½åŠ›è‡ªè©• vs å¯¦éš›çŸ¥è­˜ï¼‰
    - æ¡†æ¶æ•ˆæ‡‰æª¢æ¸¬ï¼ˆç²åˆ©vsè™§æä¸€è‡´æ€§ï¼‰
    - å¾Œæ‚”è¶¨é¿æ¸¬é‡
    - å¿ƒç†è³¬æˆ¶è­˜åˆ¥
    
    #### 2. **å‹•æ…‹Î³æ˜ å°„** (è§£æ±ºçˆ­è­°)
    
    **å•é¡Œ**ï¼š
    - Î³âˆˆ[1,4] å¤ªçª„ï¼Ÿ
    - Î³âˆˆ[0.5,10] å¤ªå¯¬ï¼Ÿ
    
    **è§£æ±ºæ–¹æ¡ˆ**ï¼šåŸºæ–¼é‡‘èç´ é¤Šå‹•æ…‹èª¿æ•´
    
    ```
    é«˜ç´ é¤Š(>75%): Î³ âˆˆ [1.0, 4.0]  (ç†æ€§)
    ä¸­ç´ é¤Š(50-75%): Î³ âˆˆ [0.5, 7.0]  (å…è¨±åå·®)
    ä½ç´ é¤Š(<50%): Î³ âˆˆ [0.5, 10.0]  (è¡Œç‚ºåèª¤å¤§)
    ```
    
    **ç†è«–ä¾æ“š**ï¼š
    - van Rooij et al. (2011, JFE)ï¼šç´ é¤Šâ†‘ â†’ æ›´ç†æ€§
    - Chetty (2006)ï¼šå¯¦è­‰Î³ä¸­ä½æ•¸=2.0
    
    #### 3. **æ­£ç¢ºCRRAæ•ˆç”¨**
    
    **Taylorå±•é–‹å«é«˜éšå‹•å·®**ï¼š
    ```
    E[U] = U(E[W]) + 0.5Â·U''Â·Var + (1/6)Â·U'''Â·Skew + (1/24)Â·U''''Â·Kurt
    ```
    
    **vs éŒ¯èª¤ç‰ˆæœ¬**ï¼š
    ```
    âŒ U = E[R] - (Î³/2)Â·ÏƒÂ²  (å¿½ç•¥skewness, kurtosis)
    ```
    
    #### 4. **å®Œæ•´çµ±è¨ˆæ¨è«–**
    
    - Newey-West HACæ¨™æº–èª¤
    - Bootstrap 95% CI
    - Holmå¤šé‡æª¢é©—æ ¡æ­£
    - Jarque-Beraå¸¸æ…‹æ€§æª¢é©—
    
    #### 5. **è¡Œç‚ºåèª¤æ•´åˆ**
    
    **Î³èª¿æ•´å…¬å¼**ï¼š
    ```
    Î³_eff = Î³_base + è¡Œç‚ºèª¿æ•´
    
    è¡Œç‚ºèª¿æ•´ = 
      -0.15 Ã— max(0, éåº¦è‡ªä¿¡ - 0.5)  (éåº¦ç©æ¥µ)
      +0.12 Ã— å¾Œæ‚”è¶¨é¿                (æ›´ä¿å®ˆ)
      +0.10 Ã— å¿ƒç†è³¬æˆ¶                (éç†æ€§ä¿å®ˆ)
      +0.15 Ã— æ¡†æ¶æ•ˆæ‡‰                (ä¸ä¸€è‡´æ‡²ç½°)
    ```
    """)

with st.expander("âš ï¸ é™åˆ¶èˆ‡å‡è¨­"):
    st.markdown("""
    ### èª å¯¦æŠ«éœ²
    
    **å³ä½¿çµ‚æ¥µç‰ˆä»æœ‰é™åˆ¶**ï¼š
    
    1. **Lucasæ‰¹åˆ¤**ï¼šæ­·å²åƒæ•¸â‰ æœªä¾†
    2. **å–®æœŸæ¨¡å‹**ï¼šæœªè€ƒæ…®å‹•æ…‹å†å¹³è¡¡
    3. **å› å­ç¼ºå¤±**ï¼šåƒ…CAPMï¼Œç„¡Fama-French
    4. **æ¨£æœ¬æœŸé–“**ï¼š1å¹´è³‡æ–™å¯èƒ½ä¸è¶³
    5. **å•å·ä¸»è§€æ€§**ï¼šè‡ªè©•å¯èƒ½ä¸æº–
    
    **æœ¬ç³»çµ±ä¸èƒ½**ï¼š
    - ä¿è­‰æœªä¾†ç¸¾æ•ˆ
    - å–ä»£å°ˆæ¥­é¡§å•
    - è€ƒæ…®å®Œæ•´ç¨…å‹™
    
    **æœ¬ç³»çµ±å¯ä»¥**ï¼š
    - æä¾›ç†è«–ä¸€è‡´çš„é¢¨éšªè©•ä¼°
    - çµ±è¨ˆåš´è¬¹çš„æ­·å²åˆ†æ
    - å€‹äººåŒ–æ•ˆç”¨æ’åº
    """)

st.divider()
st.caption(
    "ğŸ“… **ç‰ˆæœ¬**ï¼šv7.0 çµ‚æ¥µå­¸è¡“æ‰¹åˆ¤é‡æ§‹ç‰ˆ | "
    "ğŸ”¬ **æ”¹é€²**ï¼š14é¡Œå•å·+å‹•æ…‹Î³+æ­£ç¢ºCRRA+HAC+è¡Œç‚ºåèª¤ | "
    "ğŸ“š **ç†è«–**ï¼šæ•´åˆ20+ç¯‡é ‚å°–æœŸåˆŠæ–‡ç»"
)

st.caption(
    "âš ï¸ **é‡è¦è²æ˜**ï¼šæœ¬ç³»çµ±åŸºæ–¼å­¸è¡“ç ”ç©¶ï¼ŒæŠ•è³‡æ±ºç­–æ‡‰è€ƒæ…®å€‹äººå®Œæ•´è²¡å‹™ç‹€æ³ä¸¦è«®è©¢å°ˆæ¥­é¡§å•ã€‚"
)
