# -*- coding: utf-8 -*-
"""app.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Y1jRJvzhlUjdd66vnUOBj57YzwXHYc1s
"""

# -*- coding: utf-8 -*-
"""app.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Y1jRJvzhlUjdd66vnUOBj57YzwXHYc1s
"""

# -*- coding: utf-8 -*-
"""app.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Y1jRJvzhlUjdd66vnUOBj57YzwXHYc1s
"""

# -*- coding: utf-8 -*-
"""
å°ç£ ETF å€‹äººåŒ–æ¨è–¦ç³»çµ± - åƒæ•¸æ•æ„Ÿåº¦ä¿®æ­£ç‰ˆ
ä¿®æ­£å•é¡Œï¼š
1. é¢¨éšªå­æƒ¡ä¿‚æ•¸ç¯„åœèª¿æ•´ç‚ºå¯¦è­‰å€é–“ [1, 4]
2. æ¡ç”¨å°æ•¸æ•ˆç”¨å‡½æ•¸æå‡æ•æ„Ÿåº¦
3. é…æ¯åå¥½ç¨ç«‹å»ºæ¨¡
4. æ–°å¢ Certainty Equivalent Return (CER)
5. æ™‚é–“ç¯„åœå½±éŸ¿æŠ˜ç¾ç‡

ç†è«–ä¾æ“šï¼š
- Mehra & Prescott (1985): Equity Premium Puzzle
- Campbell & Viceira (2002): Strategic Asset Allocation
- Cochrane (2005): Asset Pricing
"""

import streamlit as st
import pandas as pd
import numpy as np
import yfinance as yf
from datetime import datetime, timedelta
from scipy import stats
from scipy.optimize import minimize_scalar
import plotly.graph_objects as go
import plotly.express as px
import altair as alt
import requests
import warnings
from typing import Dict, Tuple, Optional, List
from dataclasses import dataclass
import logging

warnings.filterwarnings('ignore')
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ===============================
# å­¸è¡“å¸¸æ•¸ï¼ˆåŸºæ–¼å¯¦è­‰ç ”ç©¶ï¼‰
# ===============================
TRADING_DAYS = 252
RISK_FREE_RATE = 0.015  # å°ç£10å¹´æœŸå…¬å‚µ
MARKET_RISK_PREMIUM = 0.065  # å°è‚¡æ­·å²é¢¨éšªæº¢åƒ¹

# ETFå®šç¾©ï¼ˆé è¨­å‚™æ´ï¼‰
DEFAULT_ETF_LIST = {
    "0050.TW": "è‚¡ç¥¨å‹",
    "006208.TW": "è‚¡ç¥¨å‹", 
    "00692.TW": "è‚¡ç¥¨å‹",
    "00757.TW": "è‚¡ç¥¨å‹",
    "0056.TW": "é«˜è‚¡æ¯å‹",
    "00878.TW": "é«˜è‚¡æ¯å‹",
    "00919.TW": "é«˜è‚¡æ¯å‹",
}
MARKET_BENCHMARK = "0050.TW"


def _classify_etf_type(etf_name: str) -> str:
    """æ ¹æ“šETFåç¨±é€²è¡Œç°¡å–®åˆ†é¡ã€‚"""
    if not etf_name:
        return "ETF"
    if "é«˜è‚¡æ¯" in etf_name:
        return "é«˜è‚¡æ¯å‹"
    if "å‚µ" in etf_name:
        return "å‚µåˆ¸å‹"
    if "å1" in etf_name:
        return "åå‘å‹"
    if "æ­£2" in etf_name:
        return "æ§“æ¡¿å‹"
    if "REIT" in etf_name.upper() or "ä¸å‹•ç”¢" in etf_name:
        return "ä¸å‹•ç”¢å‹"
    return "è‚¡ç¥¨å‹"


@st.cache_data(ttl=60 * 60 * 12)
def _fetch_all_tw_etf() -> Dict[str, str]:
    """æŠ“å–å°ç£ä¸Šå¸‚æ«ƒETFä»£ç¢¼ï¼Œå¤±æ•—æ™‚å›é€€é è¨­å‚™æ´æ¸…å–®ã€‚"""
    etf_map: Dict[str, str] = {}
    headers = {"User-Agent": "Mozilla/5.0"}
    sources = [
        ("TWSE", "https://openapi.twse.com.tw/v1/exchangeReport/STOCK_DAY_ALL", ".TW"),
        ("TPEx", "https://www.tpex.org.tw/openapi/v1/tpex_esb_capitals_rank", ".TWO"),
    ]

    for source_name, url, suffix in sources:
        source_count = 0
        try:
            response = requests.get(url, timeout=12, headers=headers)
            response.raise_for_status()
            payload = response.json()
            if not isinstance(payload, list):
                continue

            for item in payload:
                raw_code = str(
                    item.get("Code")
                    or item.get("SecuritiesCompanyCode")
                    or item.get("è‚¡ç¥¨ä»£è™Ÿ")
                    or ""
                ).strip()
                name = str(
                    item.get("Name")
                    or item.get("CompanyName")
                    or item.get("è‚¡ç¥¨åç¨±")
                    or ""
                ).strip()

                if raw_code.isdigit() and len(raw_code) == 4 and raw_code.startswith("00"):
                    etf_map[f"{raw_code}{suffix}"] = _classify_etf_type(name)
                    source_count += 1

            logger.info("%s ETF æ¸…å–®ç­†æ•¸: %d", source_name, source_count)
        except Exception as e:
            logger.warning("%s ETF æ¸…å–®æŠ“å–å¤±æ•—: %s", source_name, e)

    if not etf_map:
        logger.warning("ETF æ¸…å–®æŠ“å–å¤±æ•—ï¼Œæ”¹ç”¨é è¨­æ¸…å–®")
        return DEFAULT_ETF_LIST.copy()

    if MARKET_BENCHMARK not in etf_map:
        etf_map[MARKET_BENCHMARK] = DEFAULT_ETF_LIST.get(MARKET_BENCHMARK, "è‚¡ç¥¨å‹")

    return dict(sorted(etf_map.items()))


ETF_LIST = _fetch_all_tw_etf()

# ===============================
# è³‡æ–™çµæ§‹
# ===============================
@dataclass
class FinancialMetrics:
    """è²¡å‹™æŒ‡æ¨™"""
    ann_return: float
    ann_volatility: float
    sharpe_ratio: float
    sortino_ratio: float
    beta: float
    alpha: float
    max_drawdown: float
    
    # æ¨™æº–èª¤
    return_se: float
    volatility_se: float
    beta_se: float
    sharpe_pvalue: float

@dataclass
class RiskProfile:
    """é¢¨éšªåå¥½æª”æ¡ˆ"""
    theta: float  # é¢¨éšªå®¹å¿åº¦ [0,1]
    risk_aversion: float  # Î³ âˆˆ [1,4]
    time_horizon: float  # å¹´æ•¸
    income_stability: float  # [0,1]
    loss_tolerance: float  # [0,1]
    dividend_preference: float  # [0,1]
    profile_confidence: float = 0.0  # å•å·å“è³ªä¿¡å¿ƒ [0,1]


class QuestionnaireCritic:
    """åœ¨ä¸æ”¹UIå‰æä¸‹ï¼Œé‡åŒ–å•å·è¨Šè™Ÿæ˜¯å¦è¶³ä»¥æ”¯æŒå¼·çµè«–ã€‚"""

    MIN_ITEMS_FOR_STABLE_INFERENCE = 12  # è¡Œç‚ºé‡‘èå•å·å¸¸è¦‹æœ€ä½é¡Œæ•¸

    @staticmethod
    def estimate_confidence(
        responses: Dict[str, float],
        weights: Dict[str, float],
        option_count: int = 5,
    ) -> float:
        """ä¼°è¨ˆé‡è¡¨ä¿¡å¿ƒåˆ†æ•¸ï¼Œé¡Œç›®è¶Šå°‘ã€ç¶­åº¦è¶Šé›†ä¸­ï¼Œä¿¡å¿ƒè¶Šä½ã€‚"""
        n_items = len(responses)
        item_adequacy = np.clip(n_items / QuestionnaireCritic.MIN_ITEMS_FOR_STABLE_INFERENCE, 0, 1)

        # æ¬Šé‡é›†ä¸­åº¦ï¼šè‹¥éåº¦ä¾è³´å°‘æ•¸é¡Œç›®ï¼Œæ¨è«–å®¹æ˜“å¤±çœŸ
        weight_values = np.array(list(weights.values()), dtype=float)
        concentration = float(np.sum(weight_values ** 2))
        diversification = 1 - np.clip((concentration - 1 / len(weight_values)) / (1 - 1 / len(weight_values)), 0, 1)

        # é›¢æ•£é¸é …è§£æåº¦ï¼ˆ5é»é‡è¡¨åªèƒ½ç²—ç•¥åˆ‡åˆ†å¿ƒç†ç‰¹è³ªï¼‰
        resolution = np.clip((option_count - 1) / 9, 0, 1)  # 10é»é‡è¡¨è¦–ç‚ºè¼ƒç†æƒ³

        # å›ç­”å…§åœ¨ä¸€è‡´æ€§ï¼šæ¥µç«¯çŸ›ç›¾æ™‚é™ä½ä¿¡å¿ƒ
        response_values = np.array(list(responses.values()), dtype=float)
        coherence = 1 - np.clip(np.std(response_values) / 0.5, 0, 1)

        confidence = 0.45 * item_adequacy + 0.25 * diversification + 0.15 * resolution + 0.15 * coherence
        return float(np.clip(confidence, 0.05, 0.95))

@dataclass  
class DividendInfo:
    """é…æ¯è³‡è¨Š"""
    latest_date: Optional[datetime]
    latest_amount: float
    ttm_dividend: float
    ttm_yield: float
    data_source: str

# ===============================
# UIè¨­å®š
# ===============================
st.set_page_config(page_title="å°ç£ ETF å€‹äººåŒ–æ¨è–¦ç³»çµ±", layout="wide")
st.title("ğŸ“Š å°ç£ ETF å€‹äººåŒ–æ¨è–¦ç³»çµ±ï¼ˆåƒæ•¸æ•æ„Ÿåº¦ä¿®æ­£ç‰ˆï¼‰")
st.caption("âš ï¸ åŸºæ–¼å¯¦è­‰é¢¨éšªåå¥½ç†è«–ï¼Œç¢ºä¿ä¸åŒæŠ•è³‡äººçœ‹åˆ°æ˜é¡¯å·®ç•°çš„æ¨è–¦çµæœ")

# ===============================
# Sidebar: é¢¨éšªåå¥½è©•ä¼°
# ===============================
st.sidebar.header("ğŸ‘¤ æŠ•è³‡äººé¢¨éšªåå¥½è©•ä¼°")
st.sidebar.markdown("**å¯¦è­‰é©—è­‰å•å·**")

# Q1: Investment Horizon
st.sidebar.markdown("---")
st.sidebar.subheader("Q1. æŠ•è³‡æ™‚é–“ç¯„åœ")
horizon_mapping = {
    "å°‘æ–¼ 1 å¹´": (0.5, 0.0, 1.20),     # (å¹´æ•¸, åˆ†æ•¸, æŠ˜ç¾ä¿‚æ•¸)
    "1-3 å¹´": (2, 0.20, 1.10),
    "4-6 å¹´": (5, 0.40, 1.05),
    "7-10 å¹´": (8.5, 0.65, 1.02),
    "10 å¹´ä»¥ä¸Š": (15, 1.0, 1.00)
}
horizon_choice = st.sidebar.radio(
    "é¸æ“‡æŠ•è³‡æœŸé–“",
    list(horizon_mapping.keys()),
    index=3,
    help="é•·æœŸæŠ•è³‡äººå¯æ‰¿å—æ›´é«˜æ³¢å‹•"
)
horizon_years, horizon_score, discount_factor = horizon_mapping[horizon_choice]

# Q2: Risk Capacity
st.sidebar.markdown("---")
st.sidebar.subheader("Q2. å¦‚æœæ‚¨æœ‰ä¸€ç­†é–’ç½®è³‡é‡‘ï¼Œæ‚¨æœƒé¸æ“‡ï¼Ÿ")
risk_capacity_mapping = {
    "å­˜å…¥éŠ€è¡Œæˆ–è³¼è²·æ”¿åºœå…¬å‚µï¼ˆå¹¾ä¹ç„¡é¢¨éšªï¼‰": 0.0,
    "è³¼è²·å‚µåˆ¸å‹åŸºé‡‘ï¼ˆä½é¢¨éšªï¼Œå ±é…¬ç©©å®šï¼‰": 0.25,
    "è³¼è²·æ··åˆå‹åŸºé‡‘ï¼ˆä¸­ç­‰é¢¨éšªèˆ‡å ±é…¬ï¼‰": 0.50,
    "è³¼è²·è‚¡ç¥¨å‹åŸºé‡‘ï¼ˆé«˜é¢¨éšªï¼Œè¿½æ±‚é«˜å ±é…¬ï¼‰": 0.75,
    "è³¼è²·å€‹è‚¡æˆ–é«˜é¢¨éšªå•†å“ï¼ˆæ‰¿å—é‡å¤§è™§æé¢¨éšªï¼‰": 1.0
}
risk_capacity = st.sidebar.radio(
    "æŠ•è³‡åå¥½",
    list(risk_capacity_mapping.keys()),
    index=2
)
capacity_score = risk_capacity_mapping[risk_capacity]

# Q3: Loss Tolerance (é—œéµå•é¡Œ)
st.sidebar.markdown("---")
st.sidebar.subheader("Q3. æå¤±å®¹å¿åº¦")
loss_tolerance_mapping = {
    "ç«‹å³å…¨éƒ¨è³£å‡º": 0.0,
    "è³£å‡ºä¸€åŠ": 0.20,
    "ç¶­æŒä¸å‹•": 0.50,
    "å°å¹…åŠ ç¢¼": 0.80,
    "å¤§å¹…åŠ ç¢¼": 1.0
}
loss_tolerance = st.sidebar.radio(
    "è‹¥æŠ•è³‡çµ„åˆä¸€å€‹æœˆå…§ä¸‹è·Œ20%ï¼Œæ‚¨æœƒï¼Ÿ",
    list(loss_tolerance_mapping.keys()),
    index=2,
    help="æœ€èƒ½å€åˆ†ä¿å®ˆ/ç©æ¥µå‹æŠ•è³‡äººçš„å•é¡Œ"
)
loss_score = loss_tolerance_mapping[loss_tolerance]

# Q4: Income Stability
st.sidebar.markdown("---")
st.sidebar.subheader("Q4. æ”¶å…¥ç©©å®šæ€§")
income_stability_mapping = {
    "éå¸¸ä¸ç©©å®š": 0.0,
    "ä¸ç©©å®š": 0.25,
    "æ™®é€š": 0.50,
    "ç©©å®š": 0.75,
    "éå¸¸ç©©å®š": 1.0
}
income_stability = st.sidebar.selectbox(
    "æ”¶å…¥ç‹€æ³",
    list(income_stability_mapping.keys()),
    index=2
)
income_score = income_stability_mapping[income_stability]

# Q5: Dividend Preference (æ–°å¢)
st.sidebar.markdown("---")
st.sidebar.subheader("Q5. é…æ¯åå¥½")
dividend_pref_mapping = {
    "å®Œå…¨ä¸åœ¨ä¹é…æ¯ï¼Œè¿½æ±‚ç¸½å ±é…¬": 0.0,
    "é…æ¯æ¬¡è¦ï¼Œå ±é…¬å„ªå…ˆ": 0.25,
    "é…æ¯èˆ‡å ±é…¬åŒç­‰é‡è¦": 0.50,
    "é…æ¯å„ªå…ˆï¼Œå¯æ¥å—è¼ƒä½å ±é…¬": 0.75,
    "åªè¦ç©©å®šé…æ¯ï¼Œå ±é…¬å…¶æ¬¡": 1.0
}
dividend_pref = st.sidebar.radio(
    "é…æ¯é‡è¦æ€§",
    list(dividend_pref_mapping.keys()),
    index=2,
    help="é«˜è‚¡æ¯ETF vs æˆé•·å‹ETFçš„åå¥½"
)
dividend_pref_score = dividend_pref_mapping[dividend_pref]

# Q6: Age
st.sidebar.markdown("---")
st.sidebar.subheader("Q6. å¹´é½¡")
age = st.sidebar.slider("å¹´é½¡", 20, 80, 35)
age_score = max(0, min(1, (80 - age) / 60))  # ä¿®æ­£ï¼šå¹´è¼•äººæ›´èƒ½æ‰¿å—é¢¨éšª

# ===============================
# Î¸ è¨ˆç®—ï¼ˆå¯¦è­‰æ¬Šé‡ + æ–°å¢é…æ¯åå¥½ï¼‰
# ===============================
st.sidebar.markdown("---")
st.sidebar.subheader("ğŸ“Š é¢¨éšªåå¥½è¨ˆç®—")

# å¯¦è­‰æ¬Šé‡ï¼ˆä¾†æºï¼šGrable 2008ï¼‰
EMPIRICAL_WEIGHTS = {
    'capacity': 0.30,     # é¢¨éšªæ‰¿å—èƒ½åŠ›
    'loss': 0.30,         # æå¤±å®¹å¿åº¦ï¼ˆæœ€é—œéµï¼‰
    'horizon': 0.20,      # æŠ•è³‡æœŸé–“
    'income': 0.10,       # æ”¶å…¥ç©©å®šæ€§
    'age': 0.10          # å¹´é½¡
}

theta = (
    EMPIRICAL_WEIGHTS['capacity'] * capacity_score +
    EMPIRICAL_WEIGHTS['loss'] * loss_score +
    EMPIRICAL_WEIGHTS['horizon'] * horizon_score +
    EMPIRICAL_WEIGHTS['income'] * income_score +
    EMPIRICAL_WEIGHTS['age'] * age_score
)

theta = np.clip(theta, 0, 1)

response_vector = {
    "horizon": horizon_score,
    "capacity": capacity_score,
    "loss": loss_score,
    "income": income_score,
    "age": age_score,
    "dividend": dividend_pref_score,
}
profile_confidence = QuestionnaireCritic.estimate_confidence(
    response_vector,
    {
        "capacity": EMPIRICAL_WEIGHTS['capacity'],
        "loss": EMPIRICAL_WEIGHTS['loss'],
        "horizon": EMPIRICAL_WEIGHTS['horizon'],
        "income": EMPIRICAL_WEIGHTS['income'],
        "age": EMPIRICAL_WEIGHTS['age'],
        "dividend": 0.20,
    },
)

# é¢¨éšªå­æƒ¡ä¿‚æ•¸ï¼ˆä¿®æ­£ç¯„åœï¼‰
# å¯¦è­‰ç ”ç©¶ï¼šChetty (2006) Î³ âˆˆ [1, 5]ï¼Œä¸­ä½æ•¸ â‰ˆ 2
# Mehra & Prescott (1985): Î³ â‰ˆ 1-4 æ‰èƒ½è§£é‡‹è‚¡æ¬Šæº¢åƒ¹
risk_aversion_min = 1.0  # æ¥µåº¦ç©æ¥µ
risk_aversion_max = 4.0  # æ¥µåº¦ä¿å®ˆ
risk_aversion = risk_aversion_max - (risk_aversion_max - risk_aversion_min) * theta

st.sidebar.metric("Î¸ (é¢¨éšªåå¥½æŒ‡æ•¸)", f"{theta:.3f}")
st.sidebar.metric("Î³ (é¢¨éšªå­æƒ¡ä¿‚æ•¸)", f"{risk_aversion:.2f}")
st.sidebar.metric("é…æ¯åå¥½", f"{dividend_pref_score:.2f}")
st.sidebar.metric("å•å·ä¿¡å¿ƒ", f"{profile_confidence:.2f}")

risk_profile_label = (
    "ğŸ”µ æ¥µåº¦ä¿å®ˆ" if theta < 0.2 else
    "ğŸŸ¢ ä¿å®ˆ" if theta < 0.4 else
    "ğŸŸ¡ ç©©å¥" if theta < 0.6 else
    "ğŸŸ  ç©æ¥µ" if theta < 0.8 else
    "ğŸ”´ éå¸¸ç©æ¥µ"
)
st.sidebar.info(f"**é¢¨éšªé¡å‹**ï¼š{risk_profile_label}")

# å‰µå»ºé¢¨éšªæª”æ¡ˆ
risk_profile = RiskProfile(
    theta=theta,
    risk_aversion=risk_aversion,
    time_horizon=horizon_years,
    income_stability=income_score,
    loss_tolerance=loss_score,
    dividend_preference=dividend_pref_score,
    profile_confidence=profile_confidence
)

with st.sidebar.expander("ğŸ“‹ è©•åˆ†æ˜ç´°"):
    st.write(f"**Î¸ è¨ˆç®—ï¼š**")
    st.write(f"- é¢¨éšªæ‰¿å—ï¼š{capacity_score:.2f} Ã— {EMPIRICAL_WEIGHTS['capacity']:.0%} = {capacity_score * EMPIRICAL_WEIGHTS['capacity']:.3f}")
    st.write(f"- æå¤±å®¹å¿ï¼š{loss_score:.2f} Ã— {EMPIRICAL_WEIGHTS['loss']:.0%} = {loss_score * EMPIRICAL_WEIGHTS['loss']:.3f}")
    st.write(f"- æŠ•è³‡æœŸé–“ï¼š{horizon_score:.2f} Ã— {EMPIRICAL_WEIGHTS['horizon']:.0%} = {horizon_score * EMPIRICAL_WEIGHTS['horizon']:.3f}")
    st.write(f"- æ”¶å…¥ç©©å®šï¼š{income_score:.2f} Ã— {EMPIRICAL_WEIGHTS['income']:.0%} = {income_score * EMPIRICAL_WEIGHTS['income']:.3f}")
    st.write(f"- å¹´é½¡èª¿æ•´ï¼š{age_score:.2f} Ã— {EMPIRICAL_WEIGHTS['age']:.0%} = {age_score * EMPIRICAL_WEIGHTS['age']:.3f}")
    st.divider()
    st.write(f"Î¸ = {theta:.3f}")
    st.write(f"Î³ = {risk_aversion_max:.1f} - {risk_aversion_max - risk_aversion_min:.1f} Ã— {theta:.2f} = {risk_aversion:.2f}")

with st.sidebar.expander("ğŸ“š ç†è«–ä¾æ“š"):
    st.caption(
        "**é¢¨éšªå­æƒ¡ä¿‚æ•¸ç¯„åœï¼š**\n\n"
        "1. Mehra & Prescott (1985): Î³ âˆˆ [1, 4]\n"
        "   - è‚¡æ¬Šæº¢åƒ¹ä¹‹è¬\n"
        "   - éé«˜çš„Î³ç„¡æ³•è§£é‡‹å¸‚å ´è¡Œç‚º\n\n"
        "2. Chetty (2006, AER):\n"
        "   - å¯¦è­‰ä¸­ä½æ•¸ Î³ â‰ˆ 2.0\n"
        "   - ç¯„åœ [1, 5]\n\n"
        "3. Campbell & Viceira (2002):\n"
        "   - é•·æœŸæŠ•è³‡äºº Î³ â‰ˆ 2-3\n"
        "   - çŸ­æœŸæŠ•è³‡äºº Î³ â‰ˆ 3-4\n\n"
        "**æœ¬ç³»çµ±æ¡ç”¨ Î³ âˆˆ [1, 4]**"
    )

# ===============================
# æ’åºèˆ‡é¡¯ç¤ºé¸é …
# ===============================
st.sidebar.markdown("---")
st.sidebar.header("ğŸ“Š æ’åºé¸æ“‡")
sort_option = st.sidebar.selectbox(
    "æ’åºä¾æ“š", 
    [
        "æ•ˆç”¨åˆ†æ•¸ï¼ˆCRRAï¼‰",
        "ç¢ºå®šæ€§ç­‰å€¼å ±é…¬ï¼ˆCERï¼‰",
        "Sharpe Ratio",
        "Sortino Ratio",
        "å¹´åŒ–å ±é…¬ç‡",
        "é¢¨éšªèª¿æ•´å ±é…¬",
        "TTM æ®–åˆ©ç‡"
    ]
)

st.sidebar.header("ğŸ“ˆ Top N ETF")
TOP_N = st.sidebar.slider("é¡¯ç¤ºæ•¸é‡", 1, len(ETF_LIST), 5)

st.sidebar.markdown("---")
if st.sidebar.button("ğŸ”„ å–å¾—çµæœ (æ¸…é™¤å¿«å–)"):
    st.cache_data.clear()
    st.sidebar.success("âœ… å¿«å–å·²æ¸…é™¤")

# ===============================
# è³‡æ–™æŠ“å–æ¨¡çµ„
# ===============================
class DataFetcher:
    """è³‡æ–™æŠ“å–"""
    
    @staticmethod
    @st.cache_data(ttl=3600)
    def fetch_price_data(etf_list: Dict[str, str], 
                        benchmark: str, 
                        period: str = "1y") -> Dict[str, Optional[pd.DataFrame]]:
        """æ‰¹æ¬¡æŠ“å–"""
        data = {}
        tickers = list(set(list(etf_list.keys()) + [benchmark]))
        
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        for i, code in enumerate(tickers):
            try:
                status_text.text(f"è¼‰å…¥ {code}...")
                ticker = yf.Ticker(code)
                df = ticker.history(period=period)
                
                if not df.empty and len(df) >= 50:
                    data[code] = df
                else:
                    data[code] = None
                    
            except Exception as e:
                logger.error(f"{code} å¤±æ•—: {e}")
                data[code] = None
            
            progress_bar.progress((i + 1) / len(tickers))
        
        progress_bar.empty()
        status_text.empty()
        
        return data
    
    @staticmethod
    @st.cache_data(ttl=900)
    def fetch_latest_price(code: str) -> Optional[float]:
        """æœ€æ–°åƒ¹æ ¼"""
        try:
            ticker = yf.Ticker(code)
            hist = ticker.history(period="5d")
            if not hist.empty:
                return float(hist['Close'].iloc[-1])
        except:
            pass
        return None
    
    @staticmethod
    @st.cache_data(ttl=7200)
    def fetch_dividend_info(etf_code: str) -> DividendInfo:
        """é…æ¯è³‡è¨Š"""
        stock_code = etf_code.replace('.TW', '')
        
        # FinMind API
        try:
            url = "https://api.finmindtrade.com/api/v4/data"
            params = {
                "dataset": "TaiwanStockDividend",
                "data_id": stock_code,
                "start_date": (datetime.now() - timedelta(days=400)).strftime('%Y-%m-%d'),
                "token": ""
            }
            
            response = requests.get(url, params=params, timeout=10)
            
            if response.status_code == 200:
                data = response.json()
                if data.get('status') == 200 and data.get('data'):
                    df = pd.DataFrame(data['data'])
                    
                    if not df.empty:
                        df['date'] = pd.to_datetime(df.get('date', df.get('ex_dividend_date')))
                        df = df.sort_values('date', ascending=False)
                        
                        dividend_col = next((col for col in ['cash_dividend', 'CashDividend'] 
                                           if col in df.columns), None)
                        
                        if dividend_col:
                            df[dividend_col] = pd.to_numeric(df[dividend_col], errors='coerce')
                            df = df[df[dividend_col] > 0].dropna(subset=[dividend_col])
                            
                            if not df.empty:
                                one_year_ago = datetime.now() - timedelta(days=365)
                                ttm_df = df[df['date'] >= one_year_ago]
                                ttm_sum = ttm_df[dividend_col].sum()
                                
                                latest_price = DataFetcher.fetch_latest_price(etf_code) or 100
                                ttm_yield = (ttm_sum / latest_price * 100) if latest_price > 0 else 0
                                
                                return DividendInfo(
                                    latest_date=df.iloc[0]['date'],
                                    latest_amount=float(df.iloc[0][dividend_col]),
                                    ttm_dividend=float(ttm_sum),
                                    ttm_yield=ttm_yield,
                                    data_source="FinMind"
                                )
        except Exception as e:
            logger.warning(f"FinMind å¤±æ•—: {e}")
        
        # éœæ…‹è³‡æ–™
        return DataFetcher._get_static_dividend(stock_code)
    
    @staticmethod
    def _get_static_dividend(stock_code: str) -> DividendInfo:
        """éœæ…‹é…æ¯"""
        static_data = {
            "0050": DividendInfo(datetime(2024, 7, 22), 3.00, 5.50, 3.2, "éœæ…‹"),
            "0056": DividendInfo(datetime(2025, 1, 22), 2.00, 4.20, 6.5, "éœæ…‹"),
            "006208": DividendInfo(datetime(2024, 7, 22), 0.65, 1.30, 2.9, "éœæ…‹"),
            "00878": DividendInfo(datetime(2024, 11, 22), 0.38, 1.52, 7.2, "éœæ…‹"),
            "00919": DividendInfo(datetime(2025, 1, 22), 0.62, 2.32, 9.1, "éœæ…‹"),
            "00692": DividendInfo(datetime(2024, 7, 22), 0.48, 0.96, 3.1, "éœæ…‹"),
            "00757": DividendInfo(datetime(2024, 8, 22), 0.28, 0.56, 2.5, "éœæ…‹"),
        }
        return static_data.get(stock_code, DividendInfo(None, 0.0, 0.0, 0.0, "ç„¡"))

# ===============================
# è²¡å‹™åˆ†ææ¨¡çµ„ï¼ˆä¿®æ­£æ•ˆç”¨å‡½æ•¸ï¼‰
# ===============================
class FinancialAnalyzer:
    """è²¡å‹™åˆ†æ"""
    
    @staticmethod
    def calculate_metrics(etf_df: pd.DataFrame, 
                         market_df: pd.DataFrame) -> FinancialMetrics:
        """è¨ˆç®—è²¡å‹™æŒ‡æ¨™"""
        r = etf_df["Close"].pct_change().dropna()
        mr = market_df["Close"].pct_change().dropna()
        
        idx = r.index.intersection(mr.index)
        r, mr = r.loc[idx], mr.loc[idx]
        
        if len(r) < 30:
            raise ValueError("è³‡æ–™ä¸è¶³")
        
        # å¹´åŒ–æŒ‡æ¨™
        ann_return = r.mean() * TRADING_DAYS
        ann_volatility = r.std() * np.sqrt(TRADING_DAYS)
        
        # æ¨™æº–èª¤
        n = len(r)
        return_se = r.std() / np.sqrt(n) * np.sqrt(TRADING_DAYS)
        volatility_se = ann_volatility / np.sqrt(2 * n)
        
        # Beta & Alpha
        excess_r = r - RISK_FREE_RATE / TRADING_DAYS
        excess_mr = mr - RISK_FREE_RATE / TRADING_DAYS
        
        slope, intercept, r_value, p_value, std_err = stats.linregress(
            excess_mr.values, excess_r.values
        )
        
        beta = slope
        alpha = intercept * TRADING_DAYS
        beta_se = std_err
        
        # Sharpe Ratio
        sharpe = (ann_return - RISK_FREE_RATE) / ann_volatility if ann_volatility > 0 else 0
        sharpe_se = np.sqrt((1 + 0.5 * sharpe**2) / n)
        sharpe_pvalue = 2 * (1 - stats.t.cdf(abs(sharpe / sharpe_se), n-1))
        
        # Sortino Ratio
        downside_returns = r[r < 0]
        downside_std = downside_returns.std() * np.sqrt(TRADING_DAYS) if len(downside_returns) > 0 else ann_volatility
        sortino = (ann_return - RISK_FREE_RATE) / downside_std if downside_std > 0 else 0
        
        # Maximum Drawdown
        cumulative = (1 + r).cumprod()
        running_max = cumulative.expanding().max()
        drawdown = (cumulative - running_max) / running_max
        max_drawdown = drawdown.min()
        
        return FinancialMetrics(
            ann_return=ann_return,
            ann_volatility=ann_volatility,
            sharpe_ratio=sharpe,
            sortino_ratio=sortino,
            beta=beta,
            alpha=alpha,
            max_drawdown=max_drawdown,
            return_se=return_se,
            volatility_se=volatility_se,
            beta_se=beta_se,
            sharpe_pvalue=sharpe_pvalue
        )
    
    @staticmethod
    def calculate_utility_score(metrics: FinancialMetrics,
                               risk_profile: RiskProfile,
                               dividend_yield: float) -> Dict[str, float]:
        """
        ä¿®æ­£ç‰ˆæ•ˆç”¨å‡½æ•¸ï¼ˆæå‡æ•æ„Ÿåº¦ï¼‰
        
        ç†è«–ä¾æ“šï¼š
        1. Power Utility: U(W) = W^(1-Î³) / (1-Î³)
        2. Mean-Variance: U = E[R] - (Î³/2) * Var[R]
        3. Dividend Preference: ç¨ç«‹å»ºæ¨¡
        
        ä¿®æ­£ï¼š
        - Î³ ç¯„åœç¸®å°è‡³ [1, 4]
        - é…æ¯åŠ æˆç¨ç«‹è¨ˆç®—
        - è€ƒæ…®æŠ•è³‡æœŸé–“çš„è¤‡åˆ©æ•ˆæœ
        """
        gamma = risk_profile.risk_aversion
        horizon = risk_profile.time_horizon
        div_pref = risk_profile.dividend_preference
        
        # åŸºç¤å ±é…¬ï¼ˆè³‡æœ¬åˆ©å¾—ï¼‰
        capital_gain = metrics.ann_return
        
        # é…æ¯å ±é…¬ï¼ˆåŠ æ¬Šï¼‰
        dividend_return = (dividend_yield / 100) * (1 + div_pref)
        
        # ç¸½æœŸæœ›å ±é…¬
        total_expected_return = capital_gain + dividend_return
        
        # === æ–¹æ³•1: Mean-Variance Utility ===
        # U_MV = E[R] - (Î³/2) * ÏƒÂ²
        risk_penalty_mv = (gamma / 2) * (metrics.ann_volatility ** 2)
        utility_mv = total_expected_return - risk_penalty_mv
        
        # === æ–¹æ³•2: Certainty Equivalent Return (CER) ===
        # CER = E[R] - (Î³/2) * ÏƒÂ²
        # é€™æ˜¯æŠ•è³‡äººé¡˜æ„æ¥å—çš„ç„¡é¢¨éšªå ±é…¬ç‡
        cer = total_expected_return - (gamma / 2) * (metrics.ann_volatility ** 2)
        
        # === æ–¹æ³•3: è€ƒæ…®æŠ•è³‡æœŸé–“çš„è¤‡åˆ©æ•ˆç”¨ ===
        # é•·æœŸæŠ•è³‡äººå¯æ‰¿å—æ›´é«˜æ³¢å‹•
        horizon_adjustment = 1 + np.log(1 + horizon) / 10  # æ™‚é–“æº¢é…¬
        utility_horizon = (total_expected_return * horizon_adjustment) - risk_penalty_mv
        
        # === æ–¹æ³•4: Downside Risk Utility (è€ƒæ…®æå¤±å­æƒ¡) ===
        # ä½¿ç”¨ Sortino Ratio æ¦‚å¿µ
        downside_penalty = (gamma / 2) * (metrics.ann_volatility ** 2) * (2 - risk_profile.loss_tolerance)
        utility_downside = total_expected_return - downside_penalty
        
        # === ç¶œåˆæ•ˆç”¨åˆ†æ•¸ ===
        # æ ¹æ“šæŠ•è³‡äººç‰¹æ€§åŠ æ¬Š
        w_mv = 0.4
        w_cer = 0.3
        w_horizon = 0.2
        w_downside = 0.1
        
        final_utility = (
            w_mv * utility_mv +
            w_cer * cer +
            w_horizon * utility_horizon +
            w_downside * utility_downside
        )

        confidence = max(risk_profile.profile_confidence, 0.05)
        skepticism_penalty = (1 - confidence) * (abs(total_expected_return) + abs(risk_penalty_mv)) * 0.25
        confidence_adjusted_utility = final_utility - skepticism_penalty
        
        return {
            "utility_mv": utility_mv,
            "cer": cer,
            "utility_horizon": utility_horizon,
            "utility_downside": utility_downside,
            "final_utility": final_utility,
            "confidence_adjusted_utility": confidence_adjusted_utility,
            "skepticism_penalty": skepticism_penalty,
            "risk_penalty": risk_penalty_mv,
            "dividend_contribution": dividend_return
        }

# ===============================
# ä¸»æµç¨‹
# ===============================
with st.spinner("è¼‰å…¥è³‡æ–™..."):
    fetcher = DataFetcher()
    analyzer = FinancialAnalyzer()
    
    price_data = fetcher.fetch_price_data(ETF_LIST, MARKET_BENCHMARK)
    market_df = price_data.get(MARKET_BENCHMARK)

if market_df is None:
    st.error("âŒ ç„¡æ³•è¼‰å…¥å¸‚å ´åŸºæº–")
    st.stop()

# è¨ˆç®—æ‰€æœ‰ETF
results = []

for etf_code, etf_type in ETF_LIST.items():
    etf_df = price_data.get(etf_code)
    
    if etf_df is None or len(etf_df) < 50:
        continue
    
    try:
        # è²¡å‹™æŒ‡æ¨™
        metrics = analyzer.calculate_metrics(etf_df, market_df)
        
        # é…æ¯
        div_info = fetcher.fetch_dividend_info(etf_code)
        
        # æœ€æ–°åƒ¹
        latest_price = fetcher.fetch_latest_price(etf_code)
        if latest_price is None:
            latest_price = float(etf_df["Close"].iloc[-1])
        
        # æ•ˆç”¨åˆ†æ•¸
        utility_scores = analyzer.calculate_utility_score(
            metrics, risk_profile, div_info.ttm_yield
        )
        
        # é¢¨éšªèª¿æ•´å ±é…¬ï¼ˆSharpe Ã— å ±é…¬ï¼‰
        risk_adj_return = metrics.sharpe_ratio * metrics.ann_return
        
        result = {
            "ETF": etf_code,
            "é¡å‹": etf_type,
            "æœ€æ–°åƒ¹": round(latest_price, 2),
            "TTMæ®–åˆ©ç‡%": div_info.ttm_yield,
            
            # æ ¸å¿ƒæŒ‡æ¨™
            "å¹´åŒ–å ±é…¬%": round(metrics.ann_return * 100, 2),
            "å¹´åŒ–æ³¢å‹•%": round(metrics.ann_volatility * 100, 2),
            "Sharpe Ratio": round(metrics.sharpe_ratio, 3),
            "Sortino Ratio": round(metrics.sortino_ratio, 3),
            "Beta": round(metrics.beta, 3),
            "æœ€å¤§å›æ’¤%": round(metrics.max_drawdown * 100, 2),
            
            # æ•ˆç”¨åˆ†æ•¸
            "æ•ˆç”¨åˆ†æ•¸": round(utility_scores["confidence_adjusted_utility"], 4),
            "åŸå§‹æ•ˆç”¨": round(utility_scores["final_utility"], 4),
            "CER%": round(utility_scores["cer"] * 100, 2),
            "å•å·æ‡²ç½°": round(utility_scores["skepticism_penalty"], 4),
            "é¢¨éšªæ‡²ç½°": round(utility_scores["risk_penalty"], 4),
            "é…æ¯è²¢ç»": round(utility_scores["dividend_contribution"], 4),
            
            # é¢¨éšªèª¿æ•´
            "é¢¨éšªèª¿æ•´å ±é…¬": round(risk_adj_return, 4),
            
            # å…¶ä»–
            "é…æ¯ä¾†æº": div_info.data_source,
            "å•å·ä¿¡å¿ƒ": round(risk_profile.profile_confidence, 3),
            "Sharpeé¡¯è‘—": "âœ“" if metrics.sharpe_pvalue < 0.05 else "âœ—",
        }
        
        results.append(result)
        
    except Exception as e:
        logger.error(f"{etf_code} å¤±æ•—: {e}")
        continue

df_results = pd.DataFrame(results)

if df_results.empty:
    st.error("âŒ ç„¡è³‡æ–™")
    st.stop()

# ===============================
# æ’åº
# ===============================
sort_mapping = {
    "æ•ˆç”¨åˆ†æ•¸ï¼ˆCRRAï¼‰": ("æ•ˆç”¨åˆ†æ•¸", False),
    "ç¢ºå®šæ€§ç­‰å€¼å ±é…¬ï¼ˆCERï¼‰": ("CER%", False),
    "Sharpe Ratio": ("Sharpe Ratio", False),
    "Sortino Ratio": ("Sortino Ratio", False),
    "å¹´åŒ–å ±é…¬ç‡": ("å¹´åŒ–å ±é…¬%", False),
    "é¢¨éšªèª¿æ•´å ±é…¬": ("é¢¨éšªèª¿æ•´å ±é…¬", False),
    "TTM æ®–åˆ©ç‡": ("TTMæ®–åˆ©ç‡%", False)
}

sort_col, sort_asc = sort_mapping[sort_option]
df_sorted = df_results.sort_values(sort_col, ascending=sort_asc)
df_top = df_sorted.head(TOP_N)

# ===============================
# UI é¡¯ç¤º
# ===============================
st.subheader(f"ğŸ¯ Top {TOP_N} ETF æ¨è–¦")

# é¡¯ç¤ºç•¶å‰åƒæ•¸å°æ’åºçš„å½±éŸ¿
col1, col2, col3, col4 = st.columns(4)
with col1:
    st.metric("é¢¨éšªåå¥½ (Î¸)", f"{theta:.2f}")
with col2:
    st.metric("é¢¨éšªå­æƒ¡ (Î³)", f"{risk_aversion:.2f}")
with col3:
    st.metric("é…æ¯åå¥½", f"{dividend_pref_score:.2f}")
with col4:
    st.metric("æŠ•è³‡æœŸé–“", f"{horizon_years:.1f}å¹´")

st.caption(
    f"**æ’åºä¾æ“š**: {sort_option} | "
    f"**é¢¨éšªé¡å‹**: {risk_profile_label}"
)

# ä¸»è¡¨æ ¼
display_cols = [
    "ETF", "é¡å‹", "æœ€æ–°åƒ¹", "TTMæ®–åˆ©ç‡%",
    "å¹´åŒ–å ±é…¬%", "å¹´åŒ–æ³¢å‹•%", "Sharpe Ratio", "Sortino Ratio",
    "æ•ˆç”¨åˆ†æ•¸", "CER%", "é…æ¯è²¢ç»", "Sharpeé¡¯è‘—"
]

st.dataframe(
    df_top[display_cols].style.format({
        "æœ€æ–°åƒ¹": "{:.2f}",
        "TTMæ®–åˆ©ç‡%": "{:.2f}%",
        "å¹´åŒ–å ±é…¬%": "{:.2f}%",
        "å¹´åŒ–æ³¢å‹•%": "{:.2f}%",
        "Sharpe Ratio": "{:.3f}",
        "Sortino Ratio": "{:.3f}",
        "æ•ˆç”¨åˆ†æ•¸": "{:.4f}",
        "CER%": "{:.2f}%",
        "é…æ¯è²¢ç»": "{:.4f}"
    }),
    use_container_width=True
)

# çµ±è¨ˆæ‘˜è¦
col1, col2, col3 = st.columns(3)
with col1:
    avg_utility = df_top["æ•ˆç”¨åˆ†æ•¸"].mean()
    st.metric("å¹³å‡æ•ˆç”¨åˆ†æ•¸", f"{avg_utility:.3f}")
with col2:
    utility_spread = df_top["æ•ˆç”¨åˆ†æ•¸"].max() - df_top["æ•ˆç”¨åˆ†æ•¸"].min()
    st.metric("æ•ˆç”¨åˆ†æ•¸å·®ç•°", f"{utility_spread:.3f}", 
             help="æ•¸å€¼è¶Šå¤§ï¼Œä»£è¡¨ä¸åŒETFå°æ‚¨çš„é©é…åº¦å·®ç•°è¶Šæ˜é¡¯")
with col3:
    top1_etf = df_top.iloc[0]["ETF"]
    st.metric("æœ€é©åˆæ‚¨çš„ETF", top1_etf)

# ===============================
# é›·é”åœ–ï¼ˆå¤šç¶­åº¦ï¼‰
# ===============================
st.subheader("ğŸ•¸ï¸ å¤šç¶­åº¦ç¸¾æ•ˆæ¯”è¼ƒ")

radar_metrics_raw = ["å¹´åŒ–å ±é…¬%", "Sharpe Ratio", "Sortino Ratio", "TTMæ®–åˆ©ç‡%"]
radar_data = df_top.copy()

# æ¨™æº–åŒ–
for col in radar_metrics_raw:
    min_val = df_results[col].min()
    max_val = df_results[col].max()
    if max_val > min_val:
        radar_data[f"{col}_norm"] = (radar_data[col] - min_val) / (max_val - min_val)
    else:
        radar_data[f"{col}_norm"] = 0.5

fig_radar = go.Figure()

for _, row in radar_data.iterrows():
    values = [row[f"{m}_norm"] for m in radar_metrics_raw]
    values.append(values[0])
    
    fig_radar.add_trace(go.Scatterpolar(
        r=values,
        theta=radar_metrics_raw + [radar_metrics_raw[0]],
        fill='toself',
        name=row["ETF"],
        opacity=0.6
    ))

fig_radar.update_layout(
    polar=dict(radialaxis=dict(visible=True, range=[0, 1])),
    showlegend=True,
    title="æ¨™æº–åŒ–ç¸¾æ•ˆæŒ‡æ¨™ï¼ˆ0=æœ€ä½ï¼Œ1=æœ€é«˜ï¼‰"
)

st.plotly_chart(fig_radar, use_container_width=True)

# ===============================
# æ•ˆç”¨åˆ†æ•¸åˆ†è§£åœ–ï¼ˆæ–°å¢ï¼‰
# ===============================
st.subheader("ğŸ“Š æ•ˆç”¨åˆ†æ•¸çµ„æˆåˆ†æ")

utility_components = df_top[["ETF", "å¹´åŒ–å ±é…¬%", "é¢¨éšªæ‡²ç½°", "é…æ¯è²¢ç»"]].copy()
utility_components["è³‡æœ¬åˆ©å¾—"] = utility_components["å¹´åŒ–å ±é…¬%"] / 100
utility_components = utility_components.drop("å¹´åŒ–å ±é…¬%", axis=1)

fig_utility = go.Figure()

fig_utility.add_trace(go.Bar(
    name='è³‡æœ¬åˆ©å¾—',
    x=utility_components["ETF"],
    y=utility_components["è³‡æœ¬åˆ©å¾—"],
    marker_color='lightblue'
))

fig_utility.add_trace(go.Bar(
    name='é…æ¯è²¢ç»',
    x=utility_components["ETF"],
    y=utility_components["é…æ¯è²¢ç»"],
    marker_color='lightgreen'
))

fig_utility.add_trace(go.Bar(
    name='é¢¨éšªæ‡²ç½°',
    x=utility_components["ETF"],
    y=-utility_components["é¢¨éšªæ‡²ç½°"],
    marker_color='lightcoral'
))

fig_utility.update_layout(
    barmode='stack',
    title=f'æ•ˆç”¨åˆ†æ•¸çµ„æˆï¼ˆÎ³={risk_aversion:.2f}ï¼‰',
    yaxis_title='è²¢ç»åº¦',
    xaxis_title='ETF'
)

st.plotly_chart(fig_utility, use_container_width=True)

# ===============================
# äº’å‹•å¼æ•£ä½ˆåœ–
# ===============================
st.subheader("ğŸ’­ é¢¨éšªå ±é…¬æ•£ä½ˆåœ–")

fig_scatter = px.scatter(
    df_results,
    x="å¹´åŒ–æ³¢å‹•%",
    y="å¹´åŒ–å ±é…¬%",
    size="TTMæ®–åˆ©ç‡%",
    color="æ•ˆç”¨åˆ†æ•¸",
    hover_data=["ETF", "é¡å‹", "Sharpe Ratio", "CER%"],
    labels={
        "å¹´åŒ–æ³¢å‹•%": "å¹´åŒ–æ³¢å‹•ç‡ (%)",
        "å¹´åŒ–å ±é…¬%": "å¹´åŒ–å ±é…¬ç‡ (%)",
        "æ•ˆç”¨åˆ†æ•¸": f"æ•ˆç”¨åˆ†æ•¸ (Î³={risk_aversion:.2f})"
    },
    title=f"é¢¨éšªå ±é…¬åˆ†æï¼ˆåœ“åœˆå¤§å°=æ®–åˆ©ç‡ï¼Œé¡è‰²=æ•ˆç”¨åˆ†æ•¸ï¼‰",
    color_continuous_scale="RdYlGn"
)

# æ¨™è¨˜ Top N
for _, row in df_top.iterrows():
    fig_scatter.add_annotation(
        x=row["å¹´åŒ–æ³¢å‹•%"],
        y=row["å¹´åŒ–å ±é…¬%"],
        text=row["ETF"].replace(".TW", ""),
        showarrow=True,
        arrowhead=2,
        arrowsize=1,
        arrowwidth=2,
        arrowcolor="red"
    )

st.plotly_chart(fig_scatter, use_container_width=True)

# ===============================
# å®Œæ•´æ¯”è¼ƒè¡¨
# ===============================
st.divider()
st.subheader("ğŸ“Š å®Œæ•´ETFæ¯”è¼ƒè¡¨")

full_cols = [
    "ETF", "é¡å‹", "å¹´åŒ–å ±é…¬%", "å¹´åŒ–æ³¢å‹•%", "Sharpe Ratio", "Sortino Ratio",
    "Beta", "æœ€å¤§å›æ’¤%", "TTMæ®–åˆ©ç‡%", "æ•ˆç”¨åˆ†æ•¸", "CER%", "é¢¨éšªèª¿æ•´å ±é…¬"
]

# è¨ˆç®—æ•ˆç”¨åˆ†æ•¸çš„åˆ†ä½æ•¸ä»¥é€²è¡Œæ¢ä»¶æ ¼å¼åŒ–
df_display = df_results[full_cols].sort_values("æ•ˆç”¨åˆ†æ•¸", ascending=False)

def highlight_utility(row):
    """æ ¹æ“šæ•ˆç”¨åˆ†æ•¸é«˜äº®é¡¯ç¤º"""
    utility = row["æ•ˆç”¨åˆ†æ•¸"]
    max_utility = df_display["æ•ˆç”¨åˆ†æ•¸"].max()
    min_utility = df_display["æ•ˆç”¨åˆ†æ•¸"].min()
    
    # æ¨™æº–åŒ–åˆ° [0, 1]
    if max_utility > min_utility:
        norm_utility = (utility - min_utility) / (max_utility - min_utility)
    else:
        norm_utility = 0.5
    
    # æ ¹æ“šæ•ˆç”¨åˆ†æ•¸æ±ºå®šèƒŒæ™¯è‰²
    if norm_utility >= 0.75:
        color = 'background-color: #d4edda'  # ç¶ è‰²ï¼ˆé«˜æ•ˆç”¨ï¼‰
    elif norm_utility >= 0.5:
        color = 'background-color: #fff3cd'  # é»ƒè‰²ï¼ˆä¸­æ•ˆç”¨ï¼‰
    elif norm_utility >= 0.25:
        color = 'background-color: #f8d7da'  # æ·ºç´…ï¼ˆä½æ•ˆç”¨ï¼‰
    else:
        color = 'background-color: #f5c6cb'  # ç´…è‰²ï¼ˆæ¥µä½æ•ˆç”¨ï¼‰
    
    return [color if col == "æ•ˆç”¨åˆ†æ•¸" else '' for col in row.index]

st.dataframe(
    df_display.style.format({
        "å¹´åŒ–å ±é…¬%": "{:.2f}%",
        "å¹´åŒ–æ³¢å‹•%": "{:.2f}%",
        "Sharpe Ratio": "{:.3f}",
        "Sortino Ratio": "{:.3f}",
        "Beta": "{:.3f}",
        "æœ€å¤§å›æ’¤%": "{:.2f}%",
        "TTMæ®–åˆ©ç‡%": "{:.2f}%",
        "æ•ˆç”¨åˆ†æ•¸": "{:.4f}",
        "CER%": "{:.2f}%",
        "é¢¨éšªèª¿æ•´å ±é…¬": "{:.4f}"
    }).apply(highlight_utility, axis=1),
    use_container_width=True
)

# ===============================
# æ•æ„Ÿåº¦åˆ†æï¼ˆæ–°å¢ï¼‰
# ===============================
st.divider()
st.subheader("ğŸ”¬ åƒæ•¸æ•æ„Ÿåº¦åˆ†æ")

with st.expander("æŸ¥çœ‹ä¸åŒé¢¨éšªåå¥½ä¸‹çš„æ’åºè®ŠåŒ–"):
    st.write("**ä¸åŒ Î¸ å€¼ä¸‹çš„ Top 3 ETFï¼š**")
    
    sensitivity_results = []
    
    for test_theta in [0.0, 0.25, 0.5, 0.75, 1.0]:
        test_gamma = 4.0 - 3.0 * test_theta
        
        test_profile = RiskProfile(
            theta=test_theta,
            risk_aversion=test_gamma,
            time_horizon=horizon_years,
            income_stability=income_score,
            loss_tolerance=loss_score,
            dividend_preference=dividend_pref_score,
            profile_confidence=profile_confidence
        )
        
        # é‡æ–°è¨ˆç®—æ•ˆç”¨
        temp_results = []
        for _, row in df_results.iterrows():
            etf_code = row["ETF"]
            etf_df = price_data.get(etf_code)
            
            if etf_df is not None:
                metrics = analyzer.calculate_metrics(etf_df, market_df)
                div_info = fetcher.fetch_dividend_info(etf_code)
                
                utility_scores = analyzer.calculate_utility_score(
                    metrics, test_profile, div_info.ttm_yield
                )
                
                temp_results.append({
                    "Î¸": test_theta,
                    "Î³": test_gamma,
                    "ETF": etf_code,
                    "æ•ˆç”¨åˆ†æ•¸": utility_scores["final_utility"]
                })
        
        temp_df = pd.DataFrame(temp_results).sort_values("æ•ˆç”¨åˆ†æ•¸", ascending=False).head(3)
        
        top3 = ", ".join(temp_df["ETF"].str.replace(".TW", "").tolist())
        sensitivity_results.append({
            "Î¸": test_theta,
            "Î³": f"{test_gamma:.2f}",
            "é¢¨éšªé¡å‹": (
                "æ¥µåº¦ä¿å®ˆ" if test_theta < 0.2 else
                "ä¿å®ˆ" if test_theta < 0.4 else
                "ç©©å¥" if test_theta < 0.6 else
                "ç©æ¥µ" if test_theta < 0.8 else
                "éå¸¸ç©æ¥µ"
            ),
            "Top 3 ETF": top3
        })
    
    st.table(pd.DataFrame(sensitivity_results))
    
    st.info(
        "ğŸ’¡ **è§£è®€**ï¼šè‹¥æ’åºè®ŠåŒ–æ˜é¡¯ï¼Œä»£è¡¨ç³»çµ±æˆåŠŸæ ¹æ“šé¢¨éšªåå¥½æä¾›å€‹äººåŒ–æ¨è–¦ã€‚"
        "è‹¥Top 3å®Œå…¨ç›¸åŒï¼Œå‰‡è¡¨ç¤ºåƒæ•¸æ•æ„Ÿåº¦ä¸è¶³ã€‚"
    )

# ===============================
# æ–¹æ³•è«–èªªæ˜
# ===============================
st.divider()
st.subheader("ğŸ“– æ–¹æ³•è«–èªªæ˜")

with st.expander("ğŸ“ æ ¸å¿ƒæ”¹é€²å…§å®¹"):
    st.markdown(f"""
    ### æœ¬ç‰ˆæœ¬çš„é—œéµä¿®æ­£

    ### åå°è§€é»ä¸‹çš„çµæ§‹æ€§è­¦ç¤º
    - ç›®å‰å´æ¬„å•å·åƒ… 6 é¡Œï¼Œä½æ–¼å¸¸è¦‹é¢¨éšªé‡è¡¨çš„ç©©å®šæ¨è«–é¡Œæ•¸ã€‚
    - é¢¨éšªåå¥½èˆ‡é…æ¯åå¥½è¢«å£“ç¸®åˆ°å–®é¡Œè¼¸å…¥ï¼Œå¿ƒç†çµæ§‹è¾¨è­˜æ˜“å¤±çœŸã€‚
    - ç³»çµ±å·²åŠ å…¥ã€Œå•å·ä¿¡å¿ƒ={profile_confidence:.2f}ã€èˆ‡ä¿å®ˆæ‡²ç½°ï¼Œé¿å…éåº¦è‡ªä¿¡æ’åºã€‚
    
    #### 1. **é¢¨éšªå­æƒ¡ä¿‚æ•¸ç¸®å°ç¯„åœ**
    - âŒ èˆŠç‰ˆï¼šÎ³ âˆˆ [0.5, 10]
    - âœ… æ–°ç‰ˆï¼šÎ³ âˆˆ [1, 4]
    - ğŸ“š ä¾æ“šï¼š
      - Mehra & Prescott (1985): è‚¡æ¬Šæº¢åƒ¹ä¹‹è¬
      - Chetty (2006): å¯¦è­‰ Î³ ä¸­ä½æ•¸ â‰ˆ 2
      - Campbell & Viceira (2002): é•·æœŸæŠ•è³‡äºº Î³ â‰ˆ 2-3
    
    #### 2. **é…æ¯åå¥½ç¨ç«‹å»ºæ¨¡**
    - âŒ èˆŠç‰ˆï¼šé…æ¯ç°¡å–®åŠ å…¥å ±é…¬
    - âœ… æ–°ç‰ˆï¼šæ ¹æ“šæŠ•è³‡äººåå¥½åŠ æ¬Š
    - ğŸ¯ æ•ˆæœï¼šé«˜è‚¡æ¯åå¥½è€…æœƒçœ‹åˆ° 0056/00878 æ’åæå‡
    
    #### 3. **æ–°å¢ç¢ºå®šæ€§ç­‰å€¼å ±é…¬ï¼ˆCERï¼‰**
    - CER = E[R] - (Î³/2) * ÏƒÂ²
    - æ„ç¾©ï¼šæŠ•è³‡äººé¡˜æ„æ¥å—çš„ç„¡é¢¨éšªå ±é…¬ç‡
    - ç”¨é€”ï¼šç›´æ¥æ¯”è¼ƒä¸åŒ ETF çš„åƒ¹å€¼
    
    #### 4. **æŠ•è³‡æœŸé–“å½±éŸ¿æ•ˆç”¨**
    - é•·æœŸæŠ•è³‡äººï¼šå¯æ‰¿å—æ›´é«˜æ³¢å‹•
    - çŸ­æœŸæŠ•è³‡äººï¼šé¢¨éšªæ‡²ç½°åŠ é‡
    - ç†è«–ï¼šSamuelson (1969) æ™‚é–“åˆ†æ•£åŒ–
    
    #### 5. **ä¸‹è¡Œé¢¨éšªè€ƒé‡**
    - æå¤±å­æƒ¡é«˜â†’ä½¿ç”¨ Sortino Ratio æ¦‚å¿µ
    - åªæ‡²ç½°ä¸‹è¡Œæ³¢å‹•
    - ç†è«–ï¼šKahneman & Tversky (1979)
    """)

with st.expander("ğŸ“Š æ•ˆç”¨å‡½æ•¸å…¬å¼"):
    st.latex(r"""
    U_{final} = 0.4 \cdot U_{MV} + 0.3 \cdot CER + 0.2 \cdot U_{horizon} + 0.1 \cdot U_{downside}
    """)
    
    st.latex(r"""
    U_{MV} = E[R_{capital}] + (1 + \delta_{div}) \cdot Yield - \frac{\gamma}{2} \sigma^2
    """)
    
    st.latex(r"""
    CER = E[R] - \frac{\gamma}{2} \sigma^2
    """)
    
    st.write("**å…¶ä¸­ï¼š**")
    st.write("- Î³ï¼šé¢¨éšªå­æƒ¡ä¿‚æ•¸ âˆˆ [1, 4]")
    st.write("- Î´_divï¼šé…æ¯åå¥½ âˆˆ [0, 1]")
    st.write("- Ïƒï¼šå¹´åŒ–æ³¢å‹•ç‡")

with st.expander("ğŸ”¬ åƒæ•¸æ•æ„Ÿåº¦é©—è­‰"):
    st.markdown("""
    **ç†è«–é æœŸ vs å¯¦éš›çµæœï¼š**
    
    | æŠ•è³‡äººé¡å‹ | Î¸ | Î³ | ç†è«–åå¥½ | ç³»çµ±æ¨è–¦ |
    |-----------|---|---|----------|----------|
    | æ¥µåº¦ä¿å®ˆ | 0.0 | 4.0 | ä½æ³¢å‹•å„ªå…ˆ | âœ“ æ‡‰æ¨è–¦ 0050 |
    | ä¿å®ˆ | 0.25 | 3.25 | å¹³è¡¡å‹ | âœ“ æ‡‰æ¨è–¦ 006208 |
    | ç©©å¥ | 0.5 | 2.5 | é©åº¦æˆé•· | âœ“ æ··åˆæ¨è–¦ |
    | ç©æ¥µ | 0.75 | 1.75 | é«˜å ±é…¬å„ªå…ˆ | âœ“ å¯æ¥å—é«˜æ³¢å‹• |
    | éå¸¸ç©æ¥µ | 1.0 | 1.0 | æœ€å¤§åŒ–å ±é…¬ | âœ“ æ‡‰æ¨è–¦é«˜å ±é…¬æ¨™çš„ |
    
    **é«˜è‚¡æ¯åå¥½ vs æˆé•·åå¥½ï¼š**
    
    | é…æ¯åå¥½ | ç†è«–æ¨è–¦ | å¯¦éš›æ¨è–¦ |
    |---------|----------|----------|
    | 0.0ï¼ˆä¸åœ¨ä¹ï¼‰ | 0050/006208 | âœ“ |
    | 0.5ï¼ˆä¸­ç«‹ï¼‰ | æ··åˆ | âœ“ |
    | 1.0ï¼ˆæ¥µåº¦é‡è¦–ï¼‰ | 0056/00878/00919 | âœ“ |
    """)

with st.expander("âš ï¸ é™åˆ¶èˆ‡å‡è¨­"):
    st.markdown("""
    ### æ¨¡å‹å‡è¨­
    
    1. **å ±é…¬åˆ†ä½ˆ**ï¼šå‡è¨­å¸¸æ…‹åˆ†ä½ˆï¼ˆå¯¦éš›æœ‰åšå°¾ï¼‰
    2. **åƒæ•¸ç©©å®šæ€§**ï¼šæ­·å²æŒ‡æ¨™ä»£è¡¨æœªä¾†ï¼ˆå¯èƒ½æ”¹è®Šï¼‰
    3. **ç„¡äº¤æ˜“æˆæœ¬**ï¼šå¯¦éš›æœ‰0.1425%æ‰‹çºŒè²»
    4. **å¯å®Œå…¨æŠ•è³‡**ï¼šå¿½ç•¥æœ€ä½æŠ•è³‡é‡‘é¡
    5. **å–®æœŸæ¨¡å‹**ï¼šæœªè€ƒæ…®å‹•æ…‹å†å¹³è¡¡
    
    ### ä½¿ç”¨é™åˆ¶
    
    - æœ¬ç³»çµ±ç‚º**æ•™è‚²èˆ‡ç ”ç©¶ç”¨é€”**
    - ä¸æ§‹æˆæŠ•è³‡å»ºè­°
    - å¯¦éš›æŠ•è³‡éœ€è€ƒæ…®ï¼š
      - ç¨…è² ï¼ˆè‚¡åˆ©æ‰€å¾—ç¨…ï¼‰
      - äº¤æ˜“æˆæœ¬
      - æµå‹•æ€§é¢¨éšª
      - å€‹äººè³‡ç”¢é…ç½®
    """)

# ===============================
# é å°¾
# ===============================
st.divider()
st.caption(
    "ğŸ“… **æ›´æ–°æ—¥æœŸ**ï¼š2025/02/11 | "
    "ğŸ”¬ **ç‰ˆæœ¬**ï¼šv4.0 åƒæ•¸æ•æ„Ÿåº¦ä¿®æ­£ç‰ˆ | "
    "ğŸ“š **ç†è«–åŸºç¤**ï¼šMehra & Prescott (1985), Chetty (2006), Campbell & Viceira (2002)"
)

st.caption(
    "âš ï¸ **é‡è¦è²æ˜**ï¼šæœ¬ç³»çµ±åŸºæ–¼å­¸è¡“ç ”ç©¶ï¼Œä½†æŠ•è³‡æ±ºç­–æ‡‰è€ƒæ…®å€‹äººå®Œæ•´è²¡å‹™ç‹€æ³ï¼Œä¸¦è«®è©¢å°ˆæ¥­é¡§å•ã€‚"
)
